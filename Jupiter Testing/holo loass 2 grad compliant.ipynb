{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "###\n",
    "\n",
    "# During processing we want the largest dimesnion to be the one that is virtual, wheras for loss calculation which is important yet trivial on compute we want the largest possible dimensional surface over which to calulate the loss. the smaller of the two dimensions is the one that should be made virtual\n",
    "\n",
    "def holographic_transform(input_tensor, timesteps, binary_values=True):\n",
    "    # input_tensor is a 4D tensor of shape (batch_size, 1, x, y)\n",
    "\n",
    "    # add a new dimesnsion to the input tensor to make it 5D (b, c, x, y, timesteps)\n",
    "    input_tensor = input_tensor.reshape(*input_tensor.shape, new_func(timesteps))   #.unsqueeze(4)\n",
    "    print(input_tensor.shape)\n",
    "\n",
    "    print(\"L1\", input_tensor)\n",
    "    print(\"L2\", input_tensor[0].shape)\n",
    "    print(\"L3\", input_tensor[0][0].shape)\n",
    "    print(\"L4\", input_tensor[0][0][0].shape)\n",
    "    print(\"L5\", input_tensor[0][0][0][0].shape)\n",
    "    print(\"L6\", input_tensor[0][0][0][0][0])\n",
    "\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "def new_func(timesteps):\n",
    "    return timesteps\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.tensor([[[[0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "                            [0.9, 1.0, 0.2, 0.0, 0.0]]]])\n",
    "\n",
    "print(input_tensor.shape    )\n",
    "timesteps = 10\n",
    "binary_hologram = True\n",
    "\n",
    "\n",
    "print(input_tensor)\n",
    "output_tensor = holographic_transform(input_tensor, timesteps, binary_hologram)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(input_tensor)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(output_tensor)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WORKING, KEEPS 0.0'S IN THE FIRST DIMENSION AS 1.0 HITS #####\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 3.0]])\n",
    "\n",
    "def expand_data_to_dim(input_tensor):\n",
    "    # Determine the number of classes (third dimension size)\n",
    "    num_classes = int(input_tensor.max()) + 1\n",
    "\n",
    "    # Convert the input tensor to indices\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create one-hot encoded tensor\n",
    "    one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "\n",
    "    # Permute dimensions to move the new dimension to the front\n",
    "    print(one_hot_encoded.shape)\n",
    "    one_hot_encoded = one_hot_encoded.permute(2, 0, 1 )\n",
    "\n",
    "    print(one_hot_encoded.shape)\n",
    "    print(one_hot_encoded)\n",
    "\n",
    "expand_data_to_dim(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKING, AUTO REMOVES 0.0 VALUES FROM 3D ARRAY ####\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def expand_data_to_new_dim(input_tensor, timesteps=1000):\n",
    "    \"\"\"\n",
    "    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # QUANITSE VALUES IN THE TENSOR TO steps of (1/timesteps) then multiply by timesteps to arrive at integers (this simplifies to juyt * timestep then round)\n",
    "    #quantised_tensor = torch.round(input_tensor * timesteps) *100 # / timesteps\n",
    "\n",
    "    # Determine the number of classes (third dimension size)\n",
    "    num_classes = timesteps #int(input_tensor.max()) + 1\n",
    "\n",
    "    # Convert the input tensor to indices\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create a mask for non-zero values\n",
    "    non_zero_mask = input_tensor != 0\n",
    "\n",
    "    # Create one-hot encoded tensor\n",
    "    one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "    print(one_hot_encoded.shape)\n",
    "    # Apply the mask to exclude zero values\n",
    "    one_hot_encoded = one_hot_encoded * non_zero_mask.unsqueeze(-1)\n",
    "    print(one_hot_encoded.shape)\n",
    "    # Permute dimensions to move the new dimension to the front\n",
    "    one_hot_encoded = one_hot_encoded.permute(0, 1, 4, 2, 3)\n",
    "    print(one_hot_encoded.shape)\n",
    "    return one_hot_encoded\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 6.0, 3.0]]]])\n",
    "\n",
    "print(input_tensor.shape)\n",
    "timesteps = 10    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "\n",
    "expanded_tensor = expand_data_to_new_dim(input_tensor, timesteps)\n",
    "print(expanded_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUANITSATION FOR THE HOTSHOT #####\n",
    "\n",
    "input_tensor = torch.tensor([[0.0, 0.1, 0.0],\n",
    "                             [0.345, 1.0, 0.0],\n",
    "                             [0.99999, 0.876, 1.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class True3DLoss(torch.nn.Module):\n",
    "    def __init__(self, zero_weighting=1, nonzero_weighting=1, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # QUANITSE VALUES IN THE TENSOR TO steps of (1/timesteps) then multiply by timesteps to arrive at integers (this simplifies to juyt * timestep then round)\n",
    "        quantised_tensor = torch.round(input_tensor * self.timesteps) # / timesteps\n",
    "\n",
    "        # Determine the number of classes (third dimension size)\n",
    "        num_classes = self.timesteps #int(input_tensor.max()) + 1\n",
    "\n",
    "        # Convert the input tensor to indices\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Create a mask for non-zero values\n",
    "        non_zero_mask = input_tensor != 0\n",
    "\n",
    "        # Create one-hot encoded tensor\n",
    "        one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "\n",
    "        # Apply the mask to exclude zero values\n",
    "        one_hot_encoded = one_hot_encoded * non_zero_mask.unsqueeze(-1)\n",
    "\n",
    "        # Permute dimensions to move the new dimension to the front\n",
    "        one_hot_encoded = one_hot_encoded.permute(2, 0, 1)\n",
    "        \n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return vy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "def expand_data_to_new_dim(input_tensor, timesteps):\n",
    "    num_classes = timesteps\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create a new dimension for broadcasting\n",
    "    indices = indices.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    batch_size, channels, T, x, y = indices.size()  # Unpack the correct dimensions\n",
    "    one_hot_encoded = torch.zeros(batch_size, channels, num_classes, x, y)\n",
    "    one_hot_encoded.scatter_(2, indices, 1.0)\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0]]]])\n",
    "\n",
    "print(input_tensor.shape)\n",
    "timesteps = 10    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "\n",
    "# Expand the tensor to a 3D tensor#\n",
    "#input_tensor = expand_data_to_new_dim(input_tensor, timesteps)\n",
    "#print(input_tensor.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.]]]])\n",
      "tensor([[[[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_tensor = torch.tensor([[[[1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             \n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0]]]])\n",
    "\n",
    "\n",
    "input_tensor = (input_tensor * timesteps) -1\n",
    "\n",
    "input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "print(input_tensor)\n",
    "# Assuming you have 'indices' and 'num_classes' defined\n",
    "num_classes = 1000\n",
    "indices = input_tensor.long()\n",
    "\n",
    "# Reshape the indices tensor for compatibility with scatter_\n",
    "reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "# Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2))\n",
    "\n",
    "# Use scatter_ to fill the one-hot encoded tensor\n",
    "one_hot_encoded.scatter_(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "# Convert the tensor to float\n",
    "one_hot_encoded = one_hot_encoded.float()\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class True3DLoss1(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        \n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class True3DLoss2(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        super().__init__()   \n",
    "        self.timesteps = timesteps\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        \n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return true3d_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses match within the tolerance.\n",
      "Original loss: tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Modified loss: tensor(0.0020)\n"
     ]
    }
   ],
   "source": [
    "# Create random input tensors\n",
    "torch.manual_seed(42)\n",
    "timesteps = 1000\n",
    "batch_size = 10\n",
    "height = 128\n",
    "width = 88\n",
    "channels = 1\n",
    "\n",
    "reconstructed_image = torch.rand(batch_size, channels, height, width)\n",
    "target_image = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# Calculate losses using both versions of the loss function\n",
    "original_loss_fn = True3DLoss1(timesteps)\n",
    "modified_loss_fn = True3DLoss2(timesteps)\n",
    "\n",
    "original_loss = original_loss_fn(reconstructed_image, target_image)\n",
    "modified_loss = modified_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Check if the losses are approximately equal within a tolerance\n",
    "tolerance = 1e-6\n",
    "assert torch.allclose(original_loss, modified_loss, atol=tolerance), \"Losses do not match!\"\n",
    "print(\"Losses match within the tolerance.\")\n",
    "print(\"Original loss:\", original_loss)\n",
    "print(\"Modified loss:\", modified_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Loss Function Time: 84.9020 seconds\n",
      "Modified Loss Function Time: 120.9007 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Create random input tensors\n",
    "torch.manual_seed(42)\n",
    "timesteps = 1000\n",
    "batch_size = 10\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 1\n",
    "\n",
    "reconstructed_image = torch.rand(batch_size, channels, height, width)\n",
    "target_image = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# Create instances of the loss functions\n",
    "original_loss_fn = True3DLoss1(timesteps)\n",
    "modified_loss_fn = True3DLoss2(timesteps)\n",
    "\n",
    "# Define a function for timing the original loss function\n",
    "def time_original_loss():\n",
    "    original_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Define a function for timing the modified loss function\n",
    "def time_modified_loss():\n",
    "    modified_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Measure the execution time using timeit\n",
    "num_iterations = 1000\n",
    "original_time = timeit.timeit(time_original_loss, number=num_iterations)\n",
    "modified_time = timeit.timeit(time_modified_loss, number=num_iterations)\n",
    "\n",
    "print(f\"Original Loss Function Time: {original_time:.4f} seconds\")\n",
    "print(f\"Modified Loss Function Time: {modified_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:14: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_scatter\u001b[39;00m \u001b[39mimport\u001b[39;00m scatter_max\n\u001b[0;32m      4\u001b[0m src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]])\n\u001b[0;32m      5\u001b[0m index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m]])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "src = torch.tensor([[2, 0, 1, 4, 3], [0, 2, 1, 3, 4]])\n",
    "index = torch.tensor([[4, 5, 4, 2, 3], [0, 0, 2, 2, 1]])\n",
    "\n",
    "out, argmax = scatter_max(src, index, dim=-1)\n",
    "\n",
    "print(out)\n",
    "tensor([[0, 0, 4, 3, 2, 0],\n",
    "        [2, 4, 3, 0, 0, 0]])\n",
    "\n",
    "print(argmax)\n",
    "tensor([[5, 5, 3, 4, 0, 1]\n",
    "        [1, 4, 3, 5, 5, 5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class True3DLoss5(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "        scale_time = time.time() - self.start_time\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "        where_time = time.time() - self.start_time\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "        long_tensor_time = time.time() - self.start_time\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "        view_time = time.time() - self.start_time\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        zeros_time = time.time() - self.start_time\n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "        scatter_time = time.time() - self.start_time\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "        float_time = time.time() - self.start_time\n",
    "        print(f\"scale_time: {scale_time}\\n, where_time: {where_time}\\n, long_tensor_time: {long_tensor_time}\\n, view_time: {view_time}\\n, zeros_time: {zeros_time}\\n, scatter_time: {scatter_time}\\n, float_time: {float_time}\\n\")\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        start_time = time.time()\n",
    "        self.start_time = start_time\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        recon3d_time = time.time() - start_time\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        target3d_time = time.time() - start_time\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        final_time = time.time() - start_time\n",
    "        print(f\"recon3d_time: {recon3d_time}, target3d_time: {target3d_time}, true3d_loss: {true3d_loss}, final_time: {final_time}\")\n",
    "\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = True3DLoss5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_time: 0.0009996891021728516\n",
      ", where_time: 1692716077.998831\n",
      ", long_tensor_time: 0.0019991397857666016\n",
      ", view_time: 1692716077.998831\n",
      ", zeros_time: 0.10599875450134277\n",
      ", scatter_time: 1692716078.0828307\n",
      ", float_time: 0.10599875450134277\n",
      "\n",
      "scale_time: 0.20699858665466309\n",
      ", where_time: 1692716078.0048304\n",
      ", long_tensor_time: 0.20699858665466309\n",
      ", view_time: 1692716078.0048304\n",
      ", zeros_time: 0.3413519859313965\n",
      ", scatter_time: 1692716078.1438289\n",
      ", float_time: 0.3413519859313965\n",
      "\n",
      "final_time: 0.4350404739379883\n",
      "tensor(5.3267e-08, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dense_tensor1 = torch.zeros(10, 1, 128, 88)\n",
    "dense_tensor = torch.zeros(10, 1, 128, 88)\n",
    "dense_tensor[0, 0, 0, 0] = 1\n",
    "dense_tensor[0, 0, 1, 1] = 1\n",
    "dense_tensor[0, 0, 2, 2] = 1\n",
    "\n",
    "loss = loss_fn(dense_tensor, dense_tensor1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
