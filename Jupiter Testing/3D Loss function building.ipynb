{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code_Testing_Scripts.custom_loss_fn_tester import test_custom_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramLoss(torch.nn.Module):\n",
    "    def __init__(self, num_bins=256):\n",
    "        super(HistogramLoss, self).__init__()\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "    def histogram_intersection(hist1, hist2):\n",
    "        min_hist = torch.min(hist1, hist2)\n",
    "        return torch.sum(min_hist)\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        hist_input = torch.histc(input_image, bins=self.num_bins, min=0, max=255)\n",
    "        hist_target = torch.histc(target_image, bins=self.num_bins, min=0, max=255)\n",
    "\n",
    "        hist_input = hist_input / hist_input.sum()\n",
    "        hist_target = hist_target / hist_target.sum()\n",
    "\n",
    "        loss = 1 - self.histogram_intersection(hist_input, hist_target)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ffACBLoss(torch.nn.Module):\n",
    "    def __init__(self, zero_weighting=1, nonzero_weighting=1, fullframe_weighting=1):\n",
    "        \"\"\"\n",
    "        Initializes the ACB-MSE Loss Function class with weighting coefficients.\n",
    "\n",
    "        Args:\n",
    "        - zero_weighting: a scalar weighting coefficient for the MSE loss of zero pixels\n",
    "        - nonzero_weighting: a scalar weighting coefficient for the MSE loss of non-zero pixels\n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.zero_weighting = zero_weighting\n",
    "        self.nonzero_weighting = nonzero_weighting\n",
    "        self.fullframe_weighting = fullframe_weighting\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        \"\"\"\n",
    "        Calculates the weighted mean squared error (MSE) loss between target_image and reconstructed_image.\n",
    "        The loss for zero pixels in the target_image is weighted by zero_weighting, and the loss for non-zero\n",
    "        pixels is weighted by nonzero_weighting.\n",
    "\n",
    "        Args:\n",
    "        - target_image: a tensor of shape (B, C, H, W) containing the target image\n",
    "        - reconstructed_image: a tensor of shape (B, C, H, W) containing the reconstructed image\n",
    "\n",
    "        Returns:\n",
    "        - weighted_mse_loss: a scalar tensor containing the weighted MSE loss\n",
    "        \"\"\"\n",
    "        zero_mask = (target_image == 0)\n",
    "        nonzero_mask = ~zero_mask\n",
    "\n",
    "        values_zero = target_image[zero_mask]\n",
    "        values_nonzero = target_image[nonzero_mask]\n",
    "\n",
    "        corresponding_values_zero = reconstructed_image[zero_mask]\n",
    "        corresponding_values_nonzero = reconstructed_image[nonzero_mask]\n",
    "\n",
    "        zero_loss = self.mse_loss(corresponding_values_zero, values_zero)\n",
    "        nonzero_loss = self.mse_loss(corresponding_values_nonzero, values_nonzero)\n",
    "        full_frame_loss = self.mse_loss(reconstructed_image, target_image)\n",
    "\n",
    "        if torch.isnan(zero_loss):\n",
    "            zero_loss = 0\n",
    "        if torch.isnan(nonzero_loss):\n",
    "            nonzero_loss = 0\n",
    "\n",
    "        weighted_mse_loss = (self.zero_weighting * zero_loss) + (self.nonzero_weighting * nonzero_loss) + (self.fullframe_weighting * full_frame_loss)\n",
    "\n",
    "        return weighted_mse_loss\n",
    "\n",
    "class True3DLoss(torch.nn.Module):\n",
    "    def __init__(self, zero_weighting=1, nonzero_weighting=1, timesteps=100):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "        self.zero_weighting = zero_weighting\n",
    "        self.nonzero_weighting = nonzero_weighting\n",
    "\n",
    "    def ACB_MSE_Loss (self, reconstructed, target):\n",
    "        reconstructed_image = reconstructed.clone()\n",
    "        target_image = target.clone()\n",
    "        \"\"\"\n",
    "        Calculates the weighted mean squared error (MSE) loss between target_image and reconstructed_image.\n",
    "        The loss for zero pixels in the target_image is weighted by zero_weighting, and the loss for non-zero\n",
    "        pixels is weighted by nonzero_weighting.\n",
    "\n",
    "        Args:\n",
    "        - target_image: a tensor of shape (B, C, H, W) containing the target image\n",
    "        - reconstructed_image: a tensor of shape (B, C, H, W) containing the reconstructed image\n",
    "\n",
    "        Returns:\n",
    "        - weighted_mse_loss: a scalar tensor containing the weighted MSE loss\n",
    "        \"\"\"\n",
    "        zero_mask = (target_image == 0)\n",
    "        nonzero_mask = ~zero_mask\n",
    "\n",
    "        values_zero = target_image[zero_mask]\n",
    "        values_nonzero = target_image[nonzero_mask]\n",
    "\n",
    "        corresponding_values_zero = reconstructed_image[zero_mask]\n",
    "        corresponding_values_nonzero = reconstructed_image[nonzero_mask]\n",
    "\n",
    "        zero_loss = self.mse_loss(corresponding_values_zero, values_zero)\n",
    "        nonzero_loss = self.mse_loss(corresponding_values_nonzero, values_nonzero)\n",
    "\n",
    "        if torch.isnan(zero_loss):\n",
    "            zero_loss = 0\n",
    "        if torch.isnan(nonzero_loss):\n",
    "            nonzero_loss = 0\n",
    "\n",
    "        weighted_mse_loss = (self.zero_weighting * zero_loss) + (self.nonzero_weighting * nonzero_loss)\n",
    "\n",
    "        return weighted_mse_loss\n",
    "\n",
    "    def expand_data_to_new_dimPREFERED(self, input_tensor):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # QUANITSE VALUES IN THE TENSOR TO steps of (1/timesteps) then multiply by timesteps to arrive at integers (this simplifies to juyt * timestep then round)\n",
    "        #quantised_tensor = torch.round(input_tensor * self.timesteps) # / timesteps\n",
    "\n",
    "        # Determine the number of classes (third dimension size)\n",
    "        num_classes = self.timesteps #int(input_tensor.max()) + 1\n",
    "\n",
    "        # Convert the input tensor to indices\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Create a mask for non-zero values\n",
    "        #non_zero_mask = input_tensor != 0\n",
    "\n",
    "        # Create one-hot encoded tensor\n",
    "        one_hot_encoded = torch.nn.functional.one_hot(indices, num_classes=num_classes).float()\n",
    "\n",
    "        # Apply the mask to exclude zero values\n",
    "        #one_hot_encoded = one_hot_encoded * non_zero_mask.unsqueeze(-1)\n",
    "\n",
    "        # Permute dimensions to move the new dimension to the front\n",
    "        #one_hot_encoded = one_hot_encoded.permute(0, 1, 4, 2, 3)\n",
    "        \n",
    "        return one_hot_encoded\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        \n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.ACB_MSE_Loss(reconstructed_3D_view, target_3D_view)\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEWEST3dloss(torch.nn.Module):\n",
    "    def __init__(self, timesteps=100):\n",
    "\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "    \n",
    "    def transform_to_3d_coordinates(self, input_tensor):\n",
    "\n",
    "        if len(input_tensor.shape) == 1:\n",
    "            # X\n",
    "            m = input_tensor.size()[0]\n",
    "            n = 1\n",
    "            b = 1\n",
    "\n",
    "        if len(input_tensor.shape) == 2:\n",
    "            # X, Y\n",
    "            m, n = input_tensor.size()\n",
    "            b = 1\n",
    "\n",
    "        if len(input_tensor.shape) == 3:\n",
    "            # Channel, X, Y\n",
    "            c, m, n = input_tensor.size()\n",
    "            b = 1\n",
    "\n",
    "        if len(input_tensor.shape) == 4:\n",
    "            # Batch, Channel, X, Y\n",
    "            b, c, m, n = input_tensor.size()\n",
    "\n",
    "        # create a batches tensor\n",
    "        batches = torch.arange(b).repeat_interleave(m*n).view(-1,1)\n",
    "\n",
    "        # Create indices tensor\n",
    "        indices = torch.stack( torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2) \n",
    "        indices = indices.repeat(b, 1)\n",
    "\n",
    "        # Reshape the input tensor and concatenate with indices\n",
    "        output_tensor = torch.cat((batches,  indices, input_tensor.view(-1, 1)), dim=1)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_image = self.transform_to_3d_coordinates(reconstructed_image)\n",
    "        target_image = self.transform_to_3d_coordinates(target_image,)\n",
    "        true3d_loss = self.mse_loss(reconstructed_image, target_image)\n",
    "\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEWEST3dloss2(torch.nn.Module):\n",
    "    def __init__(self, b, m, n):\n",
    "\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "    \n",
    "        # create a batches tensor\n",
    "        batches = torch.arange(b).repeat_interleave(m*n).view(-1,1)\n",
    "\n",
    "        # Create indices tensor\n",
    "        indices = torch.stack( torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2) \n",
    "        indices = indices.repeat(b, 1)\n",
    "\n",
    "        self.batches = batches  \n",
    "        self.indices = indices\n",
    "\n",
    "    def transform_to_3d_coordinates(self, input_tensor):\n",
    "        # Reshape the input tensor and concatenate with indices\n",
    "        output_tensor = torch.cat((self.batches, self.indices, input_tensor.view(-1, 1)), dim=1)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_image = self.transform_to_3d_coordinates(reconstructed_image)\n",
    "        target_image = self.transform_to_3d_coordinates(target_image,)\n",
    "        true3d_loss = self.mse_loss(reconstructed_image, target_image)\n",
    "\n",
    "        return true3d_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss function: KLDivLoss\n",
      "Time taken to compute loss:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11262260001967661\n",
      "Gradient computation successful!\n",
      "Testing loss function: BCEWithLogitsLoss\n",
      "Time taken to compute loss:\n",
      "0.12547319999430329\n",
      "Gradient computation successful!\n",
      "Testing loss function: L1Loss\n",
      "Time taken to compute loss:\n",
      "0.10983630002010614\n",
      "Gradient computation successful!\n",
      "Testing loss function: MSELoss\n",
      "Time taken to compute loss:\n",
      "0.083573299983982\n",
      "Gradient computation successful!\n",
      "Testing loss function: NEWEST3dloss\n",
      "Time taken to compute loss:\n",
      "0.3586739999882411\n",
      "Gradient computation successful!\n",
      "Testing loss function: NEWESTACB3dloss2024\n",
      "Loss function cannot work with shape (x) Check loss fn code for error.\n",
      " Failed wil error message:\n",
      " Sizes of tensors must match except in dimension 1. Expected size 1000 but got size 10 for tensor number 2 in the list.\n",
      "Time taken to compute loss:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1000 but got size 100 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_custom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKLDivLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL1Loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEWEST3dloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNEWESTACB3dloss2024\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffACBLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mffACBLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32ma:\\Users\\Ada\\GitHub\\DeepClean_Repo\\Code_Testing_Scripts\\custom_loss_fn_tester.py:84\u001b[0m, in \u001b[0;36mtest_custom_loss\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target_data)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to compute loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[43mtimeit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_custom_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Test gradient computation\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\timeit.py:239\u001b[0m, in \u001b[0;36mrepeat\u001b[1;34m(stmt, setup, timer, repeat, number, globals)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepeat\u001b[39m(stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m\"\u001b[39m, timer\u001b[38;5;241m=\u001b[39mdefault_timer,\n\u001b[0;32m    237\u001b[0m            repeat\u001b[38;5;241m=\u001b[39mdefault_repeat, number\u001b[38;5;241m=\u001b[39mdefault_number, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to create Timer object and call repeat method.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\timeit.py:206\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    204\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeat):\n\u001b[1;32m--> 206\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\timeit.py:178\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    176\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<timeit-src>:6\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer, _stmt)\u001b[0m\n",
      "File \u001b[1;32ma:\\Users\\Ada\\GitHub\\DeepClean_Repo\\Code_Testing_Scripts\\custom_loss_fn_tester.py:81\u001b[0m, in \u001b[0;36mtest_custom_loss.<locals>.time_custom_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtime_custom_loss\u001b[39m():\n\u001b[0;32m     80\u001b[0m     output \u001b[38;5;241m=\u001b[39m net(input_data)\n\u001b[1;32m---> 81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mNEWESTACB3dloss2024.forward\u001b[1;34m(self, reconstructed_image, target_image)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, reconstructed_image, target_image):\n\u001b[1;32m---> 33\u001b[0m     reconstructed_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_to_3d_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     target_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_to_3d_coordinates(target_image)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# target_image is a tensor of shape (m, 4)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Example:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# target_image = np.array([[1, 2, 3, 4],\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Identify 0 values in the 4th column of the second dimension\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mNEWESTACB3dloss2024.transform_to_3d_coordinates\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_to_3d_coordinates\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_tensor):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Reshape the input tensor and concatenate with indices\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1000 but got size 100 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "test_custom_loss( torch.nn.KLDivLoss(), torch.nn.BCEWithLogitsLoss(), torch.nn.L1Loss(), torch.nn.MSELoss(), NEWEST3dloss(),NEWESTACB3dloss2024(1, 100, 10), ffACBLoss(1.0, 1.0, 1.0), ffACBLoss(0.5, 0.5, 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2503add32e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dummy_img1 = torch.zeros(1,1,10,10)\n",
    "#set 50% of pixels to hits\n",
    "dummy_img1[:,:,:,0:5] = 1.0\n",
    "plt.imshow(dummy_img1[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  aten::copy_        21.29%     362.000us        21.29%     362.000us      36.200us            10  \n",
      "                    aten::cat        18.06%     307.000us        39.47%     671.000us     167.750us             4  \n",
      "      aten::repeat_interleave         9.12%     155.000us        44.76%     761.000us     190.250us             4  \n",
      "                 aten::arange         5.41%      92.000us         9.94%     169.000us      14.083us            12  \n",
      "           aten::index_select         4.35%      74.000us         4.47%      76.000us      38.000us             2  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.700ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Instantiate your loss module\n",
    "loss_module = NEWEST3dloss()\n",
    "\n",
    "# Create some dummy input tensor\n",
    "input_tensor = torch.randn(4, 1, 64, 64)\n",
    "\n",
    "# Run the profiler on the transform_to_3d_coordinates method\n",
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    output_tensor = loss_module.forward(input_tensor, input_tensor)\n",
    "\n",
    "# Print the profiling results\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                aten::copy_        31.63%     186.000us        31.63%     186.000us      31.000us             6  \n",
      "                  aten::cat        29.59%     174.000us        68.20%     401.000us     200.500us             2  \n",
      "                  aten::sum        15.65%      92.000us        20.24%     119.000us     119.000us             1  \n",
      "             aten::mse_loss         6.46%      38.000us        30.61%     180.000us     180.000us             1  \n",
      "           aten::as_strided         3.91%      23.000us         3.91%      23.000us       2.556us             9  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 588.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Instantiate your loss module\n",
    "loss_module = NEWEST3dloss2(4, 64, 64)\n",
    "\n",
    "# Create some dummy input tensor\n",
    "input_tensor = torch.randn(4, 1, 64, 64)\n",
    "\n",
    "# Run the profiler on the transform_to_3d_coordinates method\n",
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    output_tensor = loss_module.forward(input_tensor, input_tensor)\n",
    "\n",
    "# Print the profiling results\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 4]' is invalid for input of size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m indices_3d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(three_dim_tensor, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create a 2D tensor using the indices\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m two_dim_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mindices_3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthree_dim_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthree_dim_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal 3D Tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(three_dim_tensor)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 4]' is invalid for input of size 60"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example 3D tensor\n",
    "# Replace this with your actual 3D tensor\n",
    "three_dim_tensor = torch.rand((3, 4, 5))\n",
    "\n",
    "# Get the indices of non-zero elements in the third dimension\n",
    "indices_3d = torch.nonzero(three_dim_tensor, as_tuple=False)[:, 2]\n",
    "\n",
    "# Create a 2D tensor using the indices\n",
    "two_dim_tensor = indices_3d.view(three_dim_tensor.size(0), three_dim_tensor.size(1))\n",
    "\n",
    "print(\"Original 3D Tensor:\")\n",
    "print(three_dim_tensor)\n",
    "print(\"\\n2D Tensor with Indices:\")\n",
    "print(two_dim_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 3D Tensor:\n",
      " [[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]]\n",
      "Indices 3D: tensor([0, 1, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example 3D tensor\n",
    "# Replace this with your actual 3D tensor\n",
    "three_dim_tensor = torch.ones((2, 2, 2))\n",
    "\n",
    "print(\"Original 3D Tensor:\\n\", three_dim_tensor.numpy())\n",
    "\n",
    "# Get the indices of non-zero elements in the third dimension\n",
    "indices_3d = torch.nonzero(three_dim_tensor, as_tuple=False)[:, 2]\n",
    "\n",
    "print(\"Indices 3D:\", indices_3d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 2]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape the indices to match the size of the original tensor\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m two_dim_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mindices_3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthree_dim_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthree_dim_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal 3D Tensor:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(three_dim_tensor)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 2]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "# Reshape the indices to match the size of the original tensor\n",
    "two_dim_tensor = indices_3d.view(three_dim_tensor.size(0), three_dim_tensor.size(1))\n",
    "\n",
    "print(\"Original 3D Tensor:\")\n",
    "print(three_dim_tensor)\n",
    "print(\"\\n2D Tensor with Indices:\")\n",
    "print(two_dim_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [1 2]]\n",
      "[[[0 0]\n",
      "  [1 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [0 1]]]\n",
      "(2, 2)\n",
      "(2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, 2], \n",
    "            [1, 2]])\n",
    "\n",
    "\n",
    "C = np.array([[[0, 0],\n",
    "                [1, 0]],\n",
    "\n",
    "                [[1, 1],\n",
    "                [0, 1]]])\n",
    "\n",
    "\n",
    "\n",
    "print (A)\n",
    "\n",
    "\n",
    "print (C)\n",
    "\n",
    "\n",
    "\n",
    "print (A.shape)\n",
    "\n",
    "print (C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two ideas:\n",
    "\n",
    "\n",
    "# 1) Stack the 2d matrix 'time_step' number of times and then set all non zero values that match the third dim index to 1 and those that dont to 0 \n",
    "\n",
    "\n",
    "# 2) create a tensor that is a list of 3d coordinates taken from the cube, so we no longer compare non hits atall, we just comapre the signals in both!?!?!?!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 0],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[0, 2],\n",
      "         [2, 2]]])\n"
     ]
    }
   ],
   "source": [
    "# genrate random 3D matrix tensor\n",
    "\n",
    "A = torch.randint(0, 3, (2, 2, 2))\n",
    "print (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# genrate random 2D matrix tensor\n",
    "\n",
    "B = torch.randint(0, 3, (2, 2))\n",
    "print (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn 2d tensor into a tensor of shape (4, 3) where the first dim (4) corrsesponds to each element in the 2d tensor and the second dim (3) corresponds to the 3d coordinates of that element (x index, y index, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 2]\n",
      " [0 2 3]\n",
      " [1 0 4]\n",
      " [1 1 5]\n",
      " [1 2 6]\n",
      " [2 0 7]\n",
      " [2 1 8]\n",
      " [2 2 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your 2D tensor of shape (m, n)\n",
    "input_tensor = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]])\n",
    "\n",
    "# Get the shape of the input tensor\n",
    "m, n = input_tensor.shape\n",
    "\n",
    "# Create an array of indices corresponding to the elements in the 2D tensor\n",
    "indices = np.array(list(np.ndindex((m, n))))\n",
    "\n",
    "# Reshape the input tensor and indices to get the desired output tensor shape\n",
    "output_tensor = np.hstack((indices, input_tensor.reshape(-1, 1)))\n",
    "\n",
    "# Output the resulting tensor\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 1., 2.],\n",
      "        [0., 2., 3.],\n",
      "        [1., 0., 4.],\n",
      "        [1., 1., 5.],\n",
      "        [1., 2., 6.],\n",
      "        [2., 0., 7.],\n",
      "        [2., 1., 8.],\n",
      "        [2., 2., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def transform_to_3d_coordinates(input_tensor):\n",
    "    m, n = input_tensor.size()\n",
    "\n",
    "    # Create indices tensor\n",
    "    indices = torch.stack(torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2)\n",
    "\n",
    "    # Reshape the input tensor and concatenate with indices\n",
    "    output_tensor = torch.cat((indices, input_tensor.view(-1, 1)), dim=1)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "output_tensor = transform_to_3d_coordinates(input_tensor)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]])\n",
      "torch.Size([5, 1, 3, 3])\n",
      "tensor([[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 2.],\n",
      "        [0., 0., 2., 3.],\n",
      "        [0., 1., 0., 4.],\n",
      "        [0., 1., 1., 5.],\n",
      "        [0., 1., 2., 6.],\n",
      "        [0., 2., 0., 7.],\n",
      "        [0., 2., 1., 8.],\n",
      "        [0., 2., 2., 9.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [1., 0., 1., 2.],\n",
      "        [1., 0., 2., 3.],\n",
      "        [1., 1., 0., 4.],\n",
      "        [1., 1., 1., 5.],\n",
      "        [1., 1., 2., 6.],\n",
      "        [1., 2., 0., 7.],\n",
      "        [1., 2., 1., 8.],\n",
      "        [1., 2., 2., 9.],\n",
      "        [2., 0., 0., 1.],\n",
      "        [2., 0., 1., 2.],\n",
      "        [2., 0., 2., 3.],\n",
      "        [2., 1., 0., 4.],\n",
      "        [2., 1., 1., 5.],\n",
      "        [2., 1., 2., 6.],\n",
      "        [2., 2., 0., 7.],\n",
      "        [2., 2., 1., 8.],\n",
      "        [2., 2., 2., 9.],\n",
      "        [3., 0., 0., 1.],\n",
      "        [3., 0., 1., 2.],\n",
      "        [3., 0., 2., 3.],\n",
      "        [3., 1., 0., 4.],\n",
      "        [3., 1., 1., 5.],\n",
      "        [3., 1., 2., 6.],\n",
      "        [3., 2., 0., 7.],\n",
      "        [3., 2., 1., 8.],\n",
      "        [3., 2., 2., 9.],\n",
      "        [4., 0., 0., 1.],\n",
      "        [4., 0., 1., 2.],\n",
      "        [4., 0., 2., 3.],\n",
      "        [4., 1., 0., 4.],\n",
      "        [4., 1., 1., 5.],\n",
      "        [4., 1., 2., 6.],\n",
      "        [4., 2., 0., 7.],\n",
      "        [4., 2., 1., 8.],\n",
      "        [4., 2., 2., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def transform_to_3d_coordinates(input_tensor):\n",
    "\n",
    "    if len(input_tensor.shape) == 1:\n",
    "        # X\n",
    "        m = input_tensor.size()[0]\n",
    "        n = 1\n",
    "        b = 1\n",
    "\n",
    "    if len(input_tensor.shape) == 2:\n",
    "        # X, Y\n",
    "        m, n = input_tensor.size()\n",
    "        b = 1\n",
    "\n",
    "    if len(input_tensor.shape) == 3:\n",
    "        # Channel, X, Y\n",
    "        c, m, n = input_tensor.size()\n",
    "        b = 1\n",
    "\n",
    "    if len(input_tensor.shape) == 4:\n",
    "        # Batch, Channel, X, Y\n",
    "        b, c, m, n = input_tensor.size()\n",
    "\n",
    "    # create a batches tensor\n",
    "    batches = torch.arange(b).repeat_interleave(m*n).view(-1,1)\n",
    "\n",
    "    # Create indices tensor\n",
    "    indices = torch.stack( torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2) \n",
    "    indices = indices.repeat(b, 1)\n",
    "\n",
    "    # Reshape the input tensor and concatenate with indices\n",
    "    output_tensor = torch.cat((batches,  indices, input_tensor.view(-1, 1)), dim=1)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]], dtype=torch.float32)\n",
    "\n",
    "# add a channel dim\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# add batch dim\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# make it have 5 batchs\n",
    "input_tensor = input_tensor.repeat(5,1,1,1)\n",
    "print(input_tensor)\n",
    "\n",
    "print(input_tensor.shape)\n",
    "\n",
    "output_tensor = transform_to_3d_coordinates(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TransformTo3DCoordinatesFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_tensor):\n",
    "        m, n = input_tensor.size()\n",
    "\n",
    "        # Create indices tensor\n",
    "        indices = torch.stack(torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2)\n",
    "\n",
    "        # Reshape the input tensor and concatenate with indices\n",
    "        output_tensor = torch.cat((indices, input_tensor.view(-1, 1)), dim=1)\n",
    "\n",
    "        ctx.save_for_backward(input_tensor)\n",
    "        return output_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_tensor, = ctx.saved_tensors\n",
    "        grad_input = grad_output[:, 2:].view(input_tensor.size())\n",
    "        return grad_input\n",
    "\n",
    "# Example usage within a loss function:\n",
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        transformed_tensor = TransformTo3DCoordinatesFunction.apply(input_tensor)\n",
    "        \n",
    "        # Your loss computation using the transformed_tensor\n",
    "        \n",
    "        loss = transformed_tensor.sum()  # Replace this with your actual loss computation\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Example usage:\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "loss_function = CustomLoss()\n",
    "loss = loss_function(input_tensor)\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradients with respect to the input_tensor\n",
    "gradients = input_tensor.grad\n",
    "\n",
    "print(gradients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([0, 1, 2])\n",
      "(tensor([[0, 0, 0],\n",
      "        [1, 1, 1],\n",
      "        [2, 2, 2]]), tensor([[0, 1, 2],\n",
      "        [0, 1, 2],\n",
      "        [0, 1, 2]]))\n",
      "tensor([[[0, 0],\n",
      "         [0, 1],\n",
      "         [0, 2]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 1],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[2, 0],\n",
      "         [2, 1],\n",
      "         [2, 2]]])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 2],\n",
      "        [2, 0],\n",
      "        [2, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor with values from 0 to m-1\n",
    "arange_m = torch.arange(m)\n",
    "\n",
    "print (arange_m)\n",
    "\n",
    "# Create a 1D tensor with values from 0 to n-1\n",
    "arange_n = torch.arange(n)\n",
    "\n",
    "print (arange_n)\n",
    "\n",
    "# Create a meshgrid using the arange_m and arange_n tensors\n",
    "meshgrid = torch.meshgrid(arange_m, arange_n, indexing='ij')\n",
    "print (meshgrid)\n",
    "\n",
    "# Stack the tensors along the last dimension to get a (m x n x 2) tensor\n",
    "stacked_meshgrid = torch.stack(meshgrid, dim=-1)\n",
    "print (stacked_meshgrid)\n",
    "\n",
    "# Reshape the tensor to (m*n, 2)\n",
    "indices = stacked_meshgrid.view(-1, 2)\n",
    "print (indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class NEWESTACB3dloss2024(torch.nn.Module):\n",
    "    def __init__(self, b, m, n):\n",
    "\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.zero_weighting = 1\n",
    "        self.nonzero_weighting = 1\n",
    "\n",
    "\n",
    "        # create a batches tensor\n",
    "        batches = torch.arange(b).repeat_interleave(m*n).view(-1,1)\n",
    "\n",
    "        # Create indices tensor\n",
    "        indices = torch.stack( torch.meshgrid(torch.arange(m), torch.arange(n), indexing='ij'), dim=-1).view(-1, 2) \n",
    "        indices = indices.repeat(b, 1)\n",
    "\n",
    "        self.batches = batches  \n",
    "        self.indices = indices\n",
    "\n",
    "    def transform_to_3d_coordinates(self, input_tensor):\n",
    "        # Reshape the input tensor and concatenate with indices\n",
    "        output_tensor = torch.cat((self.batches, self.indices, input_tensor.reshape(-1, 1)), dim=1)\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_image = self.transform_to_3d_coordinates(reconstructed_image)\n",
    "        target_image = self.transform_to_3d_coordinates(target_image)\n",
    "\n",
    "\n",
    "        # target_image is a tensor of shape (m, 4)\n",
    "        # Example:\n",
    "        # target_image = np.array([[1, 2, 3, 4],\n",
    "        #                         [5, 6, 7, 0],\n",
    "        #                         [8, 9, 10, 11]])\n",
    "\n",
    "        # Identify 0 values in the 4th column of the second dimension\n",
    "\n",
    "        zero_mask = (target_image[:, 3] == 0)\n",
    "        nonzero_mask = ~zero_mask\n",
    "\n",
    "        values_zero = target_image[zero_mask]\n",
    "        values_nonzero = target_image[nonzero_mask]\n",
    "\n",
    "        corresponding_values_zero = reconstructed_image[zero_mask]\n",
    "        corresponding_values_nonzero = reconstructed_image[nonzero_mask]\n",
    "\n",
    "        zero_loss = self.mse_loss(corresponding_values_zero, values_zero)\n",
    "        nonzero_loss = self.mse_loss(corresponding_values_nonzero, values_nonzero)\n",
    "\n",
    "        if torch.isnan(zero_loss):\n",
    "            zero_loss = 0\n",
    "        if torch.isnan(nonzero_loss):\n",
    "            nonzero_loss = 0\n",
    "\n",
    "        weighted_mse_loss = (self.zero_weighting * zero_loss) + (self.nonzero_weighting * nonzero_loss)\n",
    "\n",
    "        return weighted_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                    aten::cat        16.89%     418.000us        34.91%     864.000us     432.000us             2  \n",
      "                  aten::index        16.28%     403.000us        32.28%     799.000us     199.750us             4  \n",
      "                  aten::copy_        14.87%     368.000us        14.87%     368.000us      36.800us            10  \n",
      "                aten::nonzero        13.62%     337.000us        14.42%     357.000us      89.250us             4  \n",
      "               aten::mse_loss         7.31%     181.000us        16.73%     414.000us     207.000us             2  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.475ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Instantiate your loss module\n",
    "loss_module = NEWESTACB3dloss2024(4, 64, 64)\n",
    "\n",
    "# Create some dummy input tensor\n",
    "input_tensor = torch.randn(4, 1, 64, 64)\n",
    "\n",
    "# Run the profiler on the transform_to_3d_coordinates method\n",
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    output_tensor = loss_module.forward(input_tensor, input_tensor)\n",
    "\n",
    "# Print the profiling results\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
