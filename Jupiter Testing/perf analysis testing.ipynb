{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_excel_file_and_calculate_stats(data_dictionary, output_file_path, debug=False):\n",
    "    ### Excel file creator for validation results ###\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create an Excel writer object\n",
    "    excel_writer = pd.ExcelWriter(output_file_path, engine='xlsxwriter')\n",
    "\n",
    "    # Iterate through the nested dictionary and create sheets, rows, and columns\n",
    "    for sheet_name, sheet_data in data_dictionary.items():\n",
    "        df = pd.DataFrame.from_dict(sheet_data, orient='index')\n",
    "        df.to_excel(excel_writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Save the Excel file\n",
    "    excel_writer.save()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Excel file  has been created at: '{output_file_path}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Excel file formatter and statistics for validation results ###\n",
    "    # NOTE: Works great but excel will give error on opening as it has to fix some problem to do with the table formatting. i think its beacuse px only supports manually providing coloum headers unlike from inside excel natively where you can specify the table already has headers. once yuo accept the excel error message the file works perfectly including the tables. i assume it is finxing whatever issue there is automatically.\n",
    "\n",
    "    input_file_path = output_file_path\n",
    "\n",
    "    import openpyxl as px\n",
    "    from openpyxl.formatting.rule import ColorScaleRule, DataBarRule\n",
    "    from openpyxl.utils import get_column_letter\n",
    "\n",
    "    # Load the Excel file\n",
    "    workbook = px.load_workbook(input_file_path)\n",
    "\n",
    "    #%% - File by File Results Sheets ###\n",
    "    # Loop through each sheet in the Excel file\n",
    "    for sheet in workbook.sheetnames:\n",
    "        ws = workbook[sheet]\n",
    "        row_position_for_avg_cell = ws.max_row + 1\n",
    "        num_of_files = ws.max_row - 1\n",
    "        num_of_columns = ws.max_column\n",
    "\n",
    "        # Iterate through each column and calculate the average ad statistics and add them to the sheet\n",
    "        for col_num in range(1, ws.max_column + 1):\n",
    "\n",
    "            column = ws[chr(64 + col_num)]\n",
    "            \n",
    "            # Calculate the cell for the AVERAGE function\n",
    "            average_cell = f\"{chr(64 + col_num)}{row_position_for_avg_cell}\"\n",
    "            std_dev_cell = f\"{chr(64 + col_num)}{row_position_for_avg_cell + 1}\"\n",
    "            std_err_cell = f\"{chr(64 + col_num)}{row_position_for_avg_cell + 2}\"\n",
    "            conf_int_cell = f\"{chr(64 + col_num)}{row_position_for_avg_cell + 3}\"\n",
    "\n",
    "            if col_num == 1:\n",
    "                #Insert text into cell indicating average\n",
    "                ws[average_cell] = \"MEAN\"\n",
    "                ws[std_dev_cell] = \"STDEV\"\n",
    "                ws[std_err_cell] = \"SEM\"\n",
    "                ws[conf_int_cell] = \"CI\"\n",
    "\n",
    "            else:\n",
    "                # Insert the AVERAGE function into the cell\n",
    "                ws[average_cell] = f\"=AVERAGE({chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1})\"\n",
    "                ws[std_dev_cell] = f\"=STDEV({chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1})\"\n",
    "                ws[std_err_cell] = f\"=STDEV({chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1})/SQRT(COUNT({chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1}))\"\n",
    "                ws[conf_int_cell] = f\"=CONFIDENCE(0.05, STDEV({chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1}), {num_of_files})\"\n",
    "\n",
    "            # Add the conditional formatting to the column data\n",
    "            # check if coloumn is even or odd\n",
    "            if col_num % 2 == 0:\n",
    "                column_colour = px.styles.colors.Color(rgb=\"CCFFFF\")\n",
    "            else:\n",
    "                column_colour = px.styles.colors.Color(rgb=\"E5CCFF\")\n",
    "\n",
    "            # Create the data bars rule with solid fill\n",
    "            data_bars_rule = DataBarRule(\n",
    "                start_type='min',\n",
    "                end_type='max',\n",
    "                color=column_colour,  # Solid fill color\n",
    "            )\n",
    "        \n",
    "            # Add the data bars rule to the column\n",
    "            ws.conditional_formatting.add(f\"{chr(64 + col_num)}2:{chr(64 + col_num)}{row_position_for_avg_cell-1}\", data_bars_rule)\n",
    "\n",
    "\n",
    "        \n",
    "        # Add the table formatting to the sheet\n",
    "        #table_range = f\"A{1}:{chr(64 + num_of_columns)}{1 + num_of_files}\"    # Remove spaces from sheet name to create a valid table name\n",
    "        table_range = f\"A{1}:{chr(64 + num_of_columns)}{1 + num_of_files}\"\n",
    "\n",
    "        table_name = f\"Table_{sheet.replace(' ', '_')}\"\n",
    "        table = px.worksheet.table.Table(displayName=table_name, ref=table_range)\n",
    "\n",
    "        # Specify that the table has headers\n",
    "        table.tableStyleInfo = px.worksheet.table.TableStyleInfo(showFirstColumn=True)\n",
    "\n",
    "        # Add the table to the worksheet\n",
    "        ws.add_table(table)\n",
    "        \n",
    "\n",
    "\n",
    "    #%% - Model by Model Results Sheet ###\n",
    "    # Add a new sheet to the Excel file called Model by Model Results\n",
    "    model_by_model_ws = workbook.create_sheet(\"Model by Model Results\")\n",
    "\n",
    "    # Duplicate the column headers from the first sheet to thsi sheet\n",
    "    for col_num in range(1, ws.max_column + 1):\n",
    "        model_by_model_ws[f\"{chr(64 + col_num)}1\"] = ws[f\"{chr(64 + col_num)}1\"].value\n",
    "\n",
    "    # Target sheet for copy\n",
    "    target_sheet_name = \"Model by Model Results\"\n",
    "    model_by_model_ws = workbook[target_sheet_name]\n",
    "\n",
    "    # Loop through the source sheets\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        if sheet_name != target_sheet_name:\n",
    "            source_sheet = workbook[sheet_name]\n",
    "            target_row = model_by_model_ws.max_row + 1  # Find the next empty row in the target sheet\n",
    "            \n",
    "            # Define the row number from which to copy values\n",
    "            source_row_number = row_position_for_avg_cell  \n",
    "            \n",
    "            # Write the name of the source sheet into the first cell of the target row\n",
    "            model_by_model_ws[f\"A{target_row}\"] = sheet_name\n",
    "            \n",
    "            for col_num in range(2, source_sheet.max_column + 1):\n",
    "                source_cell = source_sheet.cell(row=source_row_number, column=col_num)\n",
    "\n",
    "                # Extracting the cell range\n",
    "                cell_range = (source_cell.value).split('(')[1].split(')')[0]\n",
    "\n",
    "                # Reparsing the equation to refrence the original sheet name\n",
    "                result = f\"=AVERAGE('{source_sheet.title}'!{cell_range})\"\n",
    "                target_cell = model_by_model_ws.cell(row=target_row, column=col_num, value=result)\n",
    "            \n",
    "    # in sheet model by model results, add conditional formatting to the columns\n",
    "    for col_num in range(2, model_by_model_ws.max_column + 1):\n",
    "        column = model_by_model_ws[chr(64 + col_num)]\n",
    "        # check if coloumn is even or odd\n",
    "        if col_num % 2 == 0:\n",
    "            column_colour = px.styles.colors.Color(rgb=\"CCFFFF\")\n",
    "        else:\n",
    "            column_colour = px.styles.colors.Color(rgb=\"E5CCFF\")\n",
    "\n",
    "        # Create the data bars rule with solid fill\n",
    "        data_bars_rule = DataBarRule(\n",
    "            start_type='min',\n",
    "            end_type='max',\n",
    "            color=column_colour,  # Solid fill color\n",
    "        )\n",
    "\n",
    "        # Add the data bars rule to the column\n",
    "        model_by_model_ws.conditional_formatting.add(f\"{chr(64 + col_num)}2:{chr(64 + col_num)}{model_by_model_ws.max_row}\", data_bars_rule)\n",
    "\n",
    "\n",
    "    # Add the table formatting to the sheet model by model sheet \n",
    "    table_range = f\"A{1}:{chr(64 + num_of_columns)}{model_by_model_ws.max_row}\"\n",
    "    table_name = f\"Table_Model_by_Model\"\n",
    "    table = px.worksheet.table.Table(displayName=table_name, ref=table_range)\n",
    "\n",
    "\n",
    "    # Specify that the table has headers\n",
    "    table.tableStyleInfo = px.worksheet.table.TableStyleInfo(showFirstColumn=False)\n",
    "\n",
    "    # Add the table to the worksheet\n",
    "    model_by_model_ws.add_table(table)\n",
    "\n",
    "\n",
    "    # Save the modified Excel file\n",
    "    workbook.save(output_file_path)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Modified Excel file saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%% -  Model Functions\n",
    "\n",
    "\n",
    "\n",
    "#%% -  Helper Functions\n",
    "\n",
    "def save_variable(variable, variable_name, path, force_pickle=False):\n",
    "\n",
    "    if force_pickle:\n",
    "        with open(path + variable_name + \"_forcepkl.pkl\", 'wb') as file:\n",
    "            pickle.dump(variable, file)\n",
    "    else:\n",
    "        if isinstance(variable, dict):\n",
    "            with open(path + variable_name + \"_dict.pkl\", 'wb') as file:\n",
    "                pickle.dump(variable, file)\n",
    "\n",
    "        elif isinstance(variable, np.ndarray):\n",
    "            np.save(path + variable_name + \"_array.npy\", variable)\n",
    "\n",
    "        elif isinstance(variable, torch.Tensor):\n",
    "            torch.save(variable, path + variable_name + \"_tensor.pt\")\n",
    "\n",
    "        elif isinstance(variable, list):\n",
    "            df = pd.DataFrame(variable)\n",
    "            df.to_csv(path + variable_name + \"_list.csv\", index=False)\n",
    "\n",
    "        elif isinstance(variable, int):\n",
    "            with open(path + variable_name + \"_int.pkl\", 'wb') as file:\n",
    "                pickle.dump(variable, file)\n",
    "\n",
    "        elif isinstance(variable, float):\n",
    "            with open(path + variable_name + \"_float.pkl\", 'wb') as file:\n",
    "                pickle.dump(variable, file)\n",
    "\n",
    "        elif isinstance(variable, str):\n",
    "            with open(path + variable_name + \"_str.pkl\", 'wb') as file:\n",
    "                pickle.dump(variable, file)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported variable type.\")\n",
    "\n",
    "# Helper function to clean up repeated plot save/show code\n",
    "def plot_save_choice(plot_or_save, output_file_path):\n",
    "    if plot_or_save == 0:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(output_file_path, format='png')    \n",
    "        if plot_or_save == 1:    \n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "#%% -  Evaluation Functions\n",
    "#%% Helper functions for evaluating performance of the denoiser:\n",
    "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, normalized_mutual_information, normalized_root_mse, structural_similarity, simple_metrics, variation_of_information, adapted_rand_error, tests\n",
    "\n",
    "#Signal to Noise Ratio (SNR)\n",
    "def SNR(clean_input, noised_target):\n",
    "    \"\"\"\n",
    "    Calculates the Signal to Noise Ratio (SNR) of a given signal and noise.\n",
    "    SNR is defined as the ratio of the magnitude of the signal and the magnitude of the noise.\n",
    "    \n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original signal.\n",
    "    noised_target (torch.Tensor): The signal with added noise.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated SNR value.    \n",
    "    \"\"\"\n",
    "    signal_power = torch.mean(torch.pow(clean_input, 2))\n",
    "\n",
    "    noise = clean_input - noised_target \n",
    "    noise_power = torch.mean(torch.pow(noise, 2))\n",
    "\n",
    "    snr = 10 * torch.log10(signal_power / noise_power)\n",
    "       \n",
    "    return (float(snr.numpy()))\n",
    "\n",
    "#Peak Signal-to-Noise Ratio (PSNR):\n",
    "def PSNR(clean_input, noised_target, time_dimension):\n",
    "    \"\"\"\n",
    "    Calculates the Peak Signal to Noise Ratio (PSNR) of a given image and its recovered version. PSNR is defined as the ratio of \n",
    "    the maximum possible power of a signal and the power of corrupting noise. The measure focuses on how well high-intensity \n",
    "    regions of the image come through the noise, and pays much less attention to low intensity regions.\n",
    "\n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original image.\n",
    "    noised_target (torch.Tensor): The recovered image.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated PSNR value.\n",
    "    \"\"\"\n",
    "    mse = torch.mean(torch.pow(clean_input - noised_target, 2))   #Finds the mean square error\n",
    "    max_value = time_dimension\n",
    "    psnr = 10 * torch.log10((max_value**2) / mse)\n",
    "    return (float(psnr.numpy()))\n",
    "\n",
    "#Mean Squared Error (MSE):\n",
    "def MSE(clean_input, noised_target):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE)\n",
    "\n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original image.\n",
    "    noised_target (torch.Tensor): The recovered image.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated Mean Squared Error value.\n",
    "    \"\"\"\n",
    "    mse = torch.mean(torch.pow(clean_input - noised_target, 2))\n",
    "    return (float(mse.numpy()))\n",
    "\n",
    "#Mean Absolute Error (MAE):\n",
    "def MAE(clean_input, noised_target):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error (MAE)\n",
    "\n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original image.\n",
    "    noised_target (torch.Tensor): The recovered image.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated Mean Absolute Error value.\n",
    "    \"\"\"\n",
    "    return float((torch.mean(torch.abs(clean_input - noised_target))).numpy())\n",
    "\n",
    "#Structural Similarity Index (SSIM):\n",
    "def SSIM(clean_input, noised_target):\n",
    "    \"\"\"\n",
    "    Structural Similarity Index Measure (SSIM), is a perceptual quality index that measures the structural similarity between \n",
    "    two images. SSIM takes into account the structural information of an image, such as luminance, contrast, and structure, \n",
    "    and compares the two images based on these factors. SSIM is based on a three-part similarity metric that considers the \n",
    "    structural information in the image, the dynamic range of the image, and the luminance information of the image. SSIM is \n",
    "    designed to provide a more perceptually relevant measure of image similarity than traditional metrics such as Mean Squared \n",
    "    Error or Peak Signal-to-Noise Ratio.\n",
    "\n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original image.\n",
    "    noised_target (torch.Tensor): The recovered image.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated Structural Similarity Index Measure value.\n",
    "    \"\"\"\n",
    "    clean_image = clean_input.numpy()\n",
    "    recovered_image = noised_target.numpy()\n",
    "    return structural_similarity(clean_image, recovered_image, data_range=1.0)\n",
    "\n",
    "#Correlation Coefficent\n",
    "def correlation_coeff(clean_input, noised_target):\n",
    "    \n",
    "    \"\"\"\n",
    "    Correlation coefficient is a scalar value that measures the linear relationship between two signals. The correlation \n",
    "    coefficient ranges from -1 to 1, where a value of 1 indicates a perfect positive linear relationship, a value of -1 indicates \n",
    "    a perfect negative linear relationship, and a value of 0 indicates no linear relationship between the two signals. Correlation \n",
    "    coefficient only measures the linear relationship between two signals, and does not take into account the structure of the signals.\n",
    "\n",
    "    ρ = cov(x,y) / (stddev(x) * stddev(y))\n",
    "\n",
    "    The function first computes the mean and standard deviation of each tensor, and then subtracts the mean from each element \n",
    "    to get the centered tensors x_center and y_center. The numerator is the sum of the element-wise product of x_center \n",
    "    and y_center, and the denominator is the product of the standard deviations of the two centered tensors multiplied by the \n",
    "    number of elements in the tensor. The function returns the value of the correlation coefficient ρ as the ratio of the numerator \n",
    "    and denominator.\n",
    "\n",
    "    Args:\n",
    "    clean_input (torch.Tensor): The original image.\n",
    "    noised_target (torch.Tensor): The recovered image.\n",
    "    \n",
    "    Returns:\n",
    "    The calculated correlation coefficient value.\n",
    "    \"\"\"\n",
    "    clean_mean = clean_input.mean()\n",
    "    noised_mean = noised_target.mean()\n",
    "    clean_std = clean_input.std()\n",
    "    noised_std = noised_target.std()\n",
    "    clean_center = clean_input - clean_mean\n",
    "    noised_center = noised_target - noised_mean\n",
    "    numerator = (clean_center * noised_center).sum()\n",
    "    denominator = clean_std * noised_std * clean_input.numel()\n",
    "    return float((numerator / denominator).numpy())\n",
    "\n",
    "#Mutual Information:\n",
    "def NomalisedMutualInformation(clean_input, noised_target):\n",
    "    clean_image = clean_input.detach().cpu().numpy()\n",
    "    recovered_image = noised_target.detach().cpu().numpy()\n",
    "    return normalized_mutual_information(clean_image, recovered_image)-1\n",
    "\n",
    "def compare_images_pixels(clean_img, denoised_img, terminal_print=False):   ###!!!INVESTIGATE USING PRINT = TRUE !!!!\n",
    "    clean_img = clean_img.detach().cpu().numpy()\n",
    "    denoised_img = denoised_img.detach().cpu().numpy()\n",
    "    ###TRUE HITS STATS###\n",
    "    if terminal_print:\n",
    "        print(\"###TRUE HITS STATS###\")\n",
    "    \n",
    "    ##X,Y##\n",
    "    true_hits_indexs = np.nonzero(clean_img)     # Find the indexs of the non zero pixels in clean_img\n",
    "    numof_true_hits = len(true_hits_indexs[0])   # Find the number of lit pixels in clean_img\n",
    "    if terminal_print:\n",
    "        print(\"numof_true_hits:\", numof_true_hits)\n",
    "    \n",
    "    # Check the values in corresponding indexs in denoised_img, retunr the index's and number of them that are also non zero\n",
    "    true_positive_xy_indexs = np.nonzero(denoised_img[true_hits_indexs]) \n",
    "    numof_true_positive_xy = len(true_positive_xy_indexs[0])                     # Calculate the number of pixels in clean_img that are also in denoised_img ###NUMBER OF SUCSESSFUL X,Y RECON PIXELS\n",
    "    if terminal_print:\n",
    "        print(\"numof_true_positive_xy:\", numof_true_positive_xy)\n",
    "\n",
    "    # Calculate the number of true hit pixels in clean_img that are not lit at all in denoised_img  ###NUMBER OF LOST TRUE PIXELS\n",
    "    false_negative_xy = numof_true_hits - numof_true_positive_xy\n",
    "    if terminal_print:\n",
    "        print(\"false_negative_xy:\", false_negative_xy)\n",
    "    \n",
    "    # Calculate the percentage of non zero pixels in clean_img that are also non zero in denoised_img   ###PERCENTAGE OF SUCSESSFUL X,Y RECON PIXELS\n",
    "    percentage_of_true_positive_xy = (numof_true_positive_xy / numof_true_hits) * 100\n",
    "    if terminal_print:\n",
    "        print(f\"percentage_of_true_positive_xy: {percentage_of_true_positive_xy}%\")\n",
    "    \n",
    "\n",
    "    ##TOF##\n",
    "    # Calculate the number of pixels in clean_img that are also in denoised_img and have the same TOF value  ###NUMBER OF SUCSESSFUL X,Y,TOF RECON PIXELS\n",
    "    num_of_true_positive_tof = np.count_nonzero(np.isclose(clean_img[true_hits_indexs], denoised_img[true_hits_indexs], atol=1e-4))\n",
    "    if terminal_print:\n",
    "        print(\"num_of_true_positive_tof:\", num_of_true_positive_tof)\n",
    "    \n",
    "    # Calculate the percentage of pixels in clean_img that are also in denoised_img and have the same value   ###PERCENTAGE OF SUCSESSFUL X,Y,TOF RECON PIXELS\n",
    "    percentage_of_true_positive_tof = (num_of_true_positive_tof / numof_true_hits) * 100\n",
    "    if terminal_print:\n",
    "        print(f\"percentage_of_true_positive_tof: {percentage_of_true_positive_tof}%\")    \n",
    "    \n",
    "\n",
    "    ###FALSE HIT STATS###\n",
    "    if terminal_print:\n",
    "        print(\"\\n###FALSE HIT STATS###\")        \n",
    "    clean_img_zero_indexs = np.where(clean_img == 0)   # find the index of the 0 valued pixels in clean image \n",
    "    number_of_zero_pixels = np.sum(clean_img_zero_indexs[0])   # Find the number of pixels in clean image that are zero\n",
    "    if terminal_print:\n",
    "        print(\"number_of_true_zero_pixels:\",number_of_zero_pixels)\n",
    "\n",
    "    #check the values in corresponding indexs in denoised_img, return the number of them that are non zero\n",
    "    denoised_img_false_lit_pixels = np.nonzero(denoised_img[clean_img_zero_indexs])\n",
    "    numof_false_positives_xy = len(denoised_img_false_lit_pixels[0])\n",
    "    if terminal_print:\n",
    "        print(\"numof_false_positives_xy:\",numof_false_positives_xy)\n",
    "\n",
    "    # Calculate the percentage of pixels in clean_img that are zero and are also non zero in denoised_img   ###PERCENTAGE OF FALSE LIT PIXELS\n",
    "    percentage_of_false_lit_pixels = (numof_false_positives_xy / number_of_zero_pixels) * 100\n",
    "    if terminal_print:\n",
    "        print(f\"percentage_of_false_positives_xy: {percentage_of_false_lit_pixels}%\")\n",
    "    \n",
    "    return percentage_of_true_positive_xy, percentage_of_true_positive_tof, numof_false_positives_xy\n",
    "\n",
    "def image_loader(input_image_path):\n",
    "    ### Load image from path \n",
    "    input_image = np.load(input_image_path)\n",
    "    return input_image\n",
    "\n",
    "#Combine all performance metrics into simple test script\n",
    "def quantify_performance(clean_input, noised_target, label, debug_mode=False):\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"clean_input shape, type, dtype:\", clean_input.shape, type(clean_input), clean_input.dtype)\n",
    "        print(\"noised_target shape, type, dtype:\", noised_target.shape, type(noised_target), noised_target.dtype)\n",
    "\n",
    "    performance = {}\n",
    "    performance['MSE'] = MSE(clean_input, noised_target)\n",
    "    performance['MAE'] = MAE(clean_input, noised_target)\n",
    "    performance['SNR'] = SNR(clean_input, noised_target)\n",
    "    performance['PSNR'] = PSNR(clean_input, noised_target, time_dimension=1000)\n",
    "    performance['SSIM'] = SSIM(clean_input, noised_target)\n",
    "    performance['Normalised Mutual Information'] = NomalisedMutualInformation(clean_input, noised_target)    #BROKEN\n",
    "    performance['Correlation Coefficient'] = correlation_coeff(clean_input, noised_target)\n",
    "    percentage_of_true_positive_xy, percentage_of_true_positive_tof, numof_false_positives_xy = compare_images_pixels(clean_input, noised_target)\n",
    "    performance['Percentage of true pixels lit in recon'] = percentage_of_true_positive_xy\n",
    "    performance['Percentage of true TOF pixels recon'] = percentage_of_true_positive_tof\n",
    "    performance['Number of extra lit pixels in recon'] = numof_false_positives_xy\n",
    "    return performance\n",
    "\n",
    "def custom_normalisation(data, reconstruction_threshold, time_dimension=100):\n",
    "    data = torch.where(data > 0, (((data / time_dimension) / (1/(1-reconstruction_threshold))) + reconstruction_threshold), 0 )\n",
    "    return data\n",
    "\n",
    "def custom_renormalisation(data, reconstruction_threshold, time_dimension=100):\n",
    "    data = np.where(data > reconstruction_threshold, ((data - reconstruction_threshold)*(1/(1-reconstruction_threshold)))*(time_dimension), 0)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Masking technique\n",
    "def masking_recovery(input_image, recovered_image, print_result=True, time_dimension=1000):\n",
    "    raw_input_image = input_image.copy()\n",
    "    net_recovered_image = recovered_image.copy()\n",
    "    #Evaluate usefullness \n",
    "    # count the number of non-zero values\n",
    "    masking_pixels = np.count_nonzero(net_recovered_image)\n",
    "    image_shape = net_recovered_image.shape\n",
    "    total_pixels = image_shape[0] * image_shape[1] * time_dimension\n",
    "    # print the count\n",
    "    if print_result:\n",
    "        print(f\"Total number of pixels in the timescan: {format(total_pixels, ',')}\\nNumber of pixels returned by the masking: {format(masking_pixels, ',')}\\nNumber of pixels removed from reconstruction by masking: {format(total_pixels - masking_pixels, ',')}\")\n",
    "\n",
    "    # use np.where and boolean indexing to update values in a\n",
    "    mask_indexs = np.where(net_recovered_image != 0)\n",
    "    net_recovered_image[mask_indexs] = raw_input_image[mask_indexs]\n",
    "    result = net_recovered_image\n",
    "    return result\n",
    "#%% - Testing Functions\n",
    "    \n",
    "def plot_multimodel_results(multi_model_results_dict, save_recovered_data, output_file_path):\n",
    "    print(multi_model_results_dict.keys())\n",
    "\n",
    "    # get to third level in dictionary to get metric names, (this needs a better apprach this was fastest way i coudl think for now, dlevels 1 and 2 dont matter as level 3 keys are ssaem for all level 1 and 2 keys)\n",
    "    random_key1 = random.choice(list(multi_model_results_dict.keys()))\n",
    "    print(\"random_key1:\", random_key1)\n",
    "    random_key2 = random.choice(list(multi_model_results_dict[random_key1].keys()))\n",
    "    print(\"random_key2:\", random_key2)\n",
    "    \n",
    "    # Extract all the third-level keys \n",
    "    metrics = list(multi_model_results_dict[random_key1][random_key2].keys())\n",
    "    print(\"metrics:\", metrics)\n",
    "\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(list(multi_model_results_dict.keys()))))\n",
    "    print(\"colors:\", colors )\n",
    "    for metric in metrics:\n",
    "        # Define a list of model names\n",
    "        model_names = list(multi_model_results_dict.keys())\n",
    "        print(\"model_names:\", model_names)\n",
    "        # Define a list of file names\n",
    "        file_names = [file_name for _, stats_dict in multi_model_results_dict.items() for file_name, perf_dict in stats_dict.items()]\n",
    "        print(\"file_names:\", file_names)\n",
    "        # Define a list of metric values for each model and file\n",
    "        metric_values = [[perf_dict[metric] for file_name, perf_dict in stats_dict.items()] for _, stats_dict in multi_model_results_dict.items()]\n",
    "        print(\"metric_values:\", metric_values)  \n",
    "        # Create a box plot\n",
    "        plt.boxplot(metric_values)\n",
    "        \n",
    "        # Add scatter plot\n",
    "        for i, coords in enumerate(metric_values):\n",
    "            x_coords = [i+1] * len(coords)\n",
    "            plt.scatter(x_coords, coords, color=colors[i], alpha=0.7, label=model_names[i])\n",
    "        \n",
    "        # remove x ticks\n",
    "        plt.xticks([])\n",
    "\n",
    "        # Set plot title\n",
    "        plt.title(metric)\n",
    "\n",
    "        # Set the y-axis label\n",
    "        plt.ylabel(metric)\n",
    "\n",
    "        # Show legend\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.grid(alpha=0.2)\n",
    "\n",
    "        Out_Label = output_file_path + f'Images\\{metric}.png'\n",
    "        plot_save_choice(1, Out_Label)           \n",
    "        print(f\"Plot saved to '{Out_Label}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(pretrained_model_folder_path, verbose_mode=False): # Defines a function for initialising the model. Function takes inputs, 'encoder' and 'decoder' which are expected to be classes (defining the encode and decode nets), 'encoder_path' and 'decoder_path' which are the paths to the encoder and decoder weights, and 'device' which is the device the model is to be initialised on (CPU or GPU).\n",
    "    import os\n",
    "\n",
    "    if verbose_mode:\n",
    "        print(f\"Loading deployment files from: {pretrained_model_folder_path}\")\n",
    "    # Initialize variables to store the file paths\n",
    "    py_file_path = None\n",
    "    intpkl_file_path = None\n",
    "    strpkl_file_path = None\n",
    "    pth_model_file_path = None\n",
    "\n",
    "    # Iterate through the files in the folder\n",
    "    for file_name in os.listdir(pretrained_model_folder_path):\n",
    "        file_path = os.path.join(pretrained_model_folder_path, file_name)\n",
    "        \n",
    "        if verbose_mode:\n",
    "            print(f\"Found file: {file_name}\")\n",
    "            \n",
    "        if file_name.endswith('.py'):\n",
    "            # Load the .py file\n",
    "            py_file_path = file_path\n",
    "            \n",
    "        elif file_name.endswith('_int.pkl'):\n",
    "            # Load the .pkl file\n",
    "            intpkl_file_path = file_path\n",
    "\n",
    "        elif file_name.endswith('_str.pkl'):\n",
    "            strpkl_file_path = file_path\n",
    "            \n",
    "        elif file_name.endswith('State Dicts.pth'):\n",
    "            # Load the .pth file with 'Model' in the filename\n",
    "            pth_model_file_path = file_path\n",
    "\n",
    "    # Check if all the required files are found\n",
    "    if verbose_mode:\n",
    "        if py_file_path and intpkl_file_path and strpkl_file_path and pth_model_file_path:\n",
    "            # Load the files into Python using their respective paths\n",
    "            # Add your code here to process or use the files as needed\n",
    "            print(\"Files loaded successfully!\\n\")\n",
    "        else:\n",
    "            print(\"One or more required files not found.\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Loading settings from: {intpkl_file_path} & {strpkl_file_path}\")\n",
    "    # Import the settings from the .pkl file\n",
    "    import pickle\n",
    "    with open(intpkl_file_path, 'rb') as f:\n",
    "        latent_dim = pickle.load(f)\n",
    "    latent_dim = int(latent_dim)\n",
    "\n",
    "    with open(strpkl_file_path, 'rb') as f:\n",
    "        double_precision = pickle.load(f)\n",
    "    print(\"Settings loaded successfully!\\n\")\n",
    "\n",
    "    # Import the encoder and decoder classes from the .py file\n",
    "    def import_encoder_decoder(py_file_path):\n",
    "        import importlib.util\n",
    "        module_name = os.path.splitext(os.path.basename(py_file_path))[0]\n",
    "        module_path = os.path.dirname(py_file_path)\n",
    "        if verbose_mode:\n",
    "            print(\"Module name: \", module_name + '.py')\n",
    "            print(\"Module path: \", module_path)\n",
    "        spec = importlib.util.spec_from_file_location(module_name + '.py', py_file_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        return module.Encoder, module.Decoder\n",
    "    \n",
    "    if verbose_mode:\n",
    "        print(f\"Loading Encoder and Decoder from: {py_file_path}\")\n",
    "    Encoder, Decoder = import_encoder_decoder(py_file_path)\n",
    " \n",
    "    encoder = Encoder(latent_dim, False)\n",
    "    decoder = Decoder(latent_dim, False)\n",
    "\n",
    "    # Sets the encoder and decoder to double precision floating point arithmetic (fp64)\n",
    "    if double_precision:\n",
    "        encoder.double()   \n",
    "        decoder.double()\n",
    "\n",
    "    \n",
    "    if verbose_mode:\n",
    "        print(\"Encoder and Decoder classes imported successfully!\\n\")\n",
    "\n",
    "        print(f\"Loading model weights from: {pth_model_file_path}\")\n",
    "    full_state_dict = torch.load(pth_model_file_path)                    # load saved model from disk \n",
    "    encoder.load_state_dict(full_state_dict['encoder_state_dict'])       # load the state dictionaries into the models\n",
    "    decoder.load_state_dict(full_state_dict['decoder_state_dict'])\n",
    "    if verbose_mode:\n",
    "        print(\"Model weights loaded successfully!\\n\")\n",
    "\n",
    "        print(\"Moving model to selected device:\")\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if verbose_mode:\n",
    "        print('Selected Tensor Processing Device: ' + f'{device}'.upper())  #Informs user if running on CPU or GPU\n",
    "\n",
    "    encoder.eval()                                   #.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn off them during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation\n",
    "    decoder.eval()    \n",
    "\n",
    "    encoder.to(device)   #Moves encoder to selected device, CPU/GPU\n",
    "    decoder.to(device)   #Moves decoder to selected device, CPU/GPU\n",
    "    if verbose_mode:\n",
    "        print(\"Model moved to device successfully!\\n\")\n",
    "\n",
    "    return encoder, decoder, double_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model(input_image_tensor, reconstruction_threshold, encoder, decoder, time_dimension=100):                                                        # Defines a function for generating a batch of test outputs from the autoencoder. And also the input + clean training data? Function takes inputs, 'encoder' and 'decoder' which are expected to be classes (defining the encode and decode nets), 'n' which is the number of ?????Images in the batch????, and 'noise_factor' which is a multiplier for the magnitude of the added noise allowing it to be scaled in intensity.  \n",
    "    with torch.no_grad():\n",
    "        norm_image = custom_normalisation(input_image_tensor.clone(), reconstruction_threshold, time_dimension)\n",
    "        image_prepared = norm_image.unsqueeze(0).unsqueeze(0)   #Adds two extra dimesnions to start of array so shape goes from (x,y) to (1,1,x,y) to represent batch and channel dims\n",
    "        rec_image = decoder(encoder(image_prepared))                         #Creates a recovered image (denoised image), by running a noisy image through the encoder and then the output of that through the decoder.\n",
    "        rec = rec_image.squeeze().numpy()\n",
    "        rec_image_renorm = custom_renormalisation(rec, reconstruction_threshold, time_dimension)\n",
    "        masking_rec_image = masking_recovery(input_image_tensor.numpy(), rec_image_renorm, print_result=False)\n",
    "\n",
    "    return rec_image_renorm, masking_rec_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_model(number_of_files, pretrained_model_folder_path, dataset_dir, output_file_path, plot, save_recovered_data, model_name, debug_mode, file_list):\n",
    "    encoder, decoder, double_precision = initialise_model(pretrained_model_folder_path, verbose_mode=debug_mode)\n",
    "    if debug_mode:\n",
    "        print(\"ENCODER & DECODER LOADED\")\n",
    "\n",
    "    # Create dictionary \n",
    "    files_dict_direct = {}\n",
    "    files_dict_masked = {}\n",
    "\n",
    "    ### Loop through the files and run function on each and collect image recovery stats\n",
    "    for i, file_name in tqdm(enumerate(file_list), desc='Processing Files', leave=False, colour='purple'):\n",
    "        image = image_loader(dataset_dir + '\\\\Data\\\\' + file_name)  \n",
    "        noisy_image = image_loader(dataset_dir + '\\\\Labels\\\\' + file_name)\n",
    "        noisy_image_tensor = torch.tensor(noisy_image)\n",
    "        noisy_image_batched = noisy_image_tensor      \n",
    "\n",
    "        recovered, image_masked_rec = run_model(noisy_image_batched, reconstruction_threshold=0.5, encoder=encoder, decoder=decoder, time_dimension=1000)\n",
    "\n",
    "        if save_recovered_data:\n",
    "            np.save(output_file_path + f'Recovered_data\\Direct\\{model_name} {file_name}', np.array(recovered))\n",
    "            np.save(output_file_path + f'Recovered_data\\Masking\\{model_name} {file_name}', np.array(image_masked_rec))\n",
    "        \n",
    "        #run stats collection functions\n",
    "        if debug_mode:\n",
    "            print(\"STATS COLLECTION STARTED\")\n",
    "        direct = quantify_performance(torch.tensor(image), torch.tensor(recovered), \"Direct Recovery\", debug_mode)\n",
    "        masking = quantify_performance(torch.tensor(image), torch.tensor(image_masked_rec), \"Masking\", debug_mode)\n",
    "        if debug_mode:\n",
    "            print(\"COMPLETE\")\n",
    "\n",
    "            print(\"PLOTTING STARTED\")\n",
    "        if plot != False:\n",
    "            if i < plot:\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "                axes[0].imshow(image)\n",
    "                axes[0].set_title('Input Image')\n",
    "\n",
    "                axes[1].imshow(noisy_image)\n",
    "                axes[1].set_title('Noisy Image')\n",
    "\n",
    "                axes[2].imshow(recovered)\n",
    "                axes[2].set_title('Recovered Image')\n",
    "\n",
    "                Out_Label = output_file_path + f'Comparison_Images\\{model_name} {file_name[:-4]}.png'\n",
    "                plot_save_choice(1, Out_Label)   \n",
    "        if debug_mode:\n",
    "            print(\"COMPLETE\")\n",
    "            \n",
    "        # add the results to the dictionary with the model name as the key\n",
    "        files_dict_direct[file_name] = direct\n",
    "        files_dict_masked[file_name] = masking\n",
    "\n",
    "    return (files_dict_direct, files_dict_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_perf_tests(num_files, plot, save_recovered_data, dataset_dir, output_file_path, model_names, pretrained_model_folder_paths, terminal_print, debug_mode = False):\n",
    "    # Create output path if not already in existance\n",
    "    os.makedirs(output_file_path + 'Recovered_data\\\\Direct\\\\', exist_ok=True)\n",
    "    os.makedirs(output_file_path + 'Recovered_data\\\\Masking\\\\', exist_ok=True)\n",
    "    os.makedirs(output_file_path + 'Images\\\\', exist_ok=True)\n",
    "    os.makedirs(output_file_path + 'Comparison_Images\\\\', exist_ok=True)\n",
    "    os.makedirs(output_file_path + 'Raw_performance_data\\\\', exist_ok=True)\n",
    "\n",
    "    # Create a dictionary to store the results for each model\n",
    "    multi_model_results_dict = {} # create an empty dictionary to store the results for each model\n",
    "\n",
    "    # Create file list of all the file names from the \\Data folder\n",
    "    file_list = [f for f in os.listdir(dataset_dir + '\\\\Data') if f.endswith('.npy')][:num_files]\n",
    "\n",
    "    # Loop through each model and run the test\n",
    "    for i, model_name in tqdm(enumerate(model_names), desc=\"Testing Model\", colour='pink', leave=False):\n",
    "        print(f\"Running Model: {model_name}\")\n",
    "        files_dict_direct, files_dict_masked = test_single_model(num_files, pretrained_model_folder_paths[i], dataset_dir, output_file_path, plot, save_recovered_data, model_name, debug_mode, file_list)\n",
    "        multi_model_results_dict[model_name + ' direct'] = files_dict_direct\n",
    "        multi_model_results_dict[model_name + ' masked'] = files_dict_masked\n",
    "    print(\"Data Gathering Completed\")\n",
    "\n",
    "    print(\"Analysing Results\")\n",
    "    # Save the results\n",
    "    if save_recovered_data:\n",
    "        save_variable(multi_model_results_dict, 'multi_model_results_dict', output_file_path + 'Raw_performance_data\\\\')\n",
    "        if debug_mode:\n",
    "            print(\"Raw Data Dictionary Saved\")\n",
    "\n",
    "    # Export the results to custom formatted excel file\n",
    "    excel_file_path = output_file_path + 'Raw_performance_data\\\\' + 'Hyperparameter Validation Results.xlsx'\n",
    "    create_excel_file_and_calculate_stats(multi_model_results_dict, excel_file_path, debug=False)    \n",
    "    # Plots the results\n",
    "    plot_multimodel_results(multi_model_results_dict, save_recovered_data, output_file_path)\n",
    "\n",
    "\n",
    "run_full_perf_tests(num_files=3, \n",
    "                     plot=3, \n",
    "                     save_recovered_data=True, \n",
    "                     dataset_dir=(r\"N:\\\\Yr 3 Project Datasets\\\\PERF VALIDATION SETS\\\\10K 100N 30S\\\\\"), \n",
    "                     output_file_path=r'Remove\\\\', \n",
    "                     model_names=[\"RDT 500K 1000ToF timed\", \"RDT 500K 1000ToF timed2\"], \n",
    "                     pretrained_model_folder_paths=[r'N:\\Yr 3 Project Results\\RDT 500K 1000ToF timed - Training Results\\Model_Deployment\\\\', r'N:\\Yr 3 Project Results\\RDT 500K 1000ToF timed - Training Results\\Model_Deployment\\\\'], \n",
    "                     terminal_print = True, \n",
    "                     debug_mode = False)\n",
    "\n",
    "print(\"Analysis Finished\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
