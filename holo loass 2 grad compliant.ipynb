{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "###\n",
    "\n",
    "# During processing we want the largest dimesnion to be the one that is virtual, wheras for loss calculation which is important yet trivial on compute we want the largest possible dimensional surface over which to calulate the loss. the smaller of the two dimensions is the one that should be made virtual\n",
    "\n",
    "def holographic_transform(input_tensor, timesteps, binary_values=True):\n",
    "    # input_tensor is a 4D tensor of shape (batch_size, 1, x, y)\n",
    "\n",
    "    # add a new dimesnsion to the input tensor to make it 5D (b, c, x, y, timesteps)\n",
    "    input_tensor = input_tensor.reshape(*input_tensor.shape, new_func(timesteps))   #.unsqueeze(4)\n",
    "    print(input_tensor.shape)\n",
    "\n",
    "    print(\"L1\", input_tensor)\n",
    "    print(\"L2\", input_tensor[0].shape)\n",
    "    print(\"L3\", input_tensor[0][0].shape)\n",
    "    print(\"L4\", input_tensor[0][0][0].shape)\n",
    "    print(\"L5\", input_tensor[0][0][0][0].shape)\n",
    "    print(\"L6\", input_tensor[0][0][0][0][0])\n",
    "\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "def new_func(timesteps):\n",
    "    return timesteps\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.tensor([[[[0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "                            [0.9, 1.0, 0.2, 0.0, 0.0]]]])\n",
    "\n",
    "print(input_tensor.shape    )\n",
    "timesteps = 10\n",
    "binary_hologram = True\n",
    "\n",
    "\n",
    "print(input_tensor)\n",
    "output_tensor = holographic_transform(input_tensor, timesteps, binary_hologram)\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(input_tensor)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(output_tensor)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WORKING, KEEPS 0.0'S IN THE FIRST DIMENSION AS 1.0 HITS #####\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 3.0]])\n",
    "\n",
    "def expand_data_to_dim(input_tensor):\n",
    "    # Determine the number of classes (third dimension size)\n",
    "    num_classes = int(input_tensor.max()) + 1\n",
    "\n",
    "    # Convert the input tensor to indices\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create one-hot encoded tensor\n",
    "    one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "\n",
    "    # Permute dimensions to move the new dimension to the front\n",
    "    print(one_hot_encoded.shape)\n",
    "    one_hot_encoded = one_hot_encoded.permute(2, 0, 1 )\n",
    "\n",
    "    print(one_hot_encoded.shape)\n",
    "    print(one_hot_encoded)\n",
    "\n",
    "expand_data_to_dim(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORKING, AUTO REMOVES 0.0 VALUES FROM 3D ARRAY ####\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def expand_data_to_new_dim(input_tensor, timesteps=1000):\n",
    "    \"\"\"\n",
    "    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # QUANITSE VALUES IN THE TENSOR TO steps of (1/timesteps) then multiply by timesteps to arrive at integers (this simplifies to juyt * timestep then round)\n",
    "    #quantised_tensor = torch.round(input_tensor * timesteps) *100 # / timesteps\n",
    "\n",
    "    # Determine the number of classes (third dimension size)\n",
    "    num_classes = timesteps #int(input_tensor.max()) + 1\n",
    "\n",
    "    # Convert the input tensor to indices\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create a mask for non-zero values\n",
    "    non_zero_mask = input_tensor != 0\n",
    "\n",
    "    # Create one-hot encoded tensor\n",
    "    one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "    print(one_hot_encoded.shape)\n",
    "    # Apply the mask to exclude zero values\n",
    "    one_hot_encoded = one_hot_encoded * non_zero_mask.unsqueeze(-1)\n",
    "    print(one_hot_encoded.shape)\n",
    "    # Permute dimensions to move the new dimension to the front\n",
    "    one_hot_encoded = one_hot_encoded.permute(0, 1, 4, 2, 3)\n",
    "    print(one_hot_encoded.shape)\n",
    "    return one_hot_encoded\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 2.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 6.0, 3.0]]]])\n",
    "\n",
    "print(input_tensor.shape)\n",
    "timesteps = 10    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "\n",
    "expanded_tensor = expand_data_to_new_dim(input_tensor, timesteps)\n",
    "print(expanded_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QUANITSATION FOR THE HOTSHOT #####\n",
    "\n",
    "input_tensor = torch.tensor([[0.0, 0.1, 0.0],\n",
    "                             [0.345, 1.0, 0.0],\n",
    "                             [0.99999, 0.876, 1.0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class True3DLoss(torch.nn.Module):\n",
    "    def __init__(self, zero_weighting=1, nonzero_weighting=1, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # QUANITSE VALUES IN THE TENSOR TO steps of (1/timesteps) then multiply by timesteps to arrive at integers (this simplifies to juyt * timestep then round)\n",
    "        quantised_tensor = torch.round(input_tensor * self.timesteps) # / timesteps\n",
    "\n",
    "        # Determine the number of classes (third dimension size)\n",
    "        num_classes = self.timesteps #int(input_tensor.max()) + 1\n",
    "\n",
    "        # Convert the input tensor to indices\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Create a mask for non-zero values\n",
    "        non_zero_mask = input_tensor != 0\n",
    "\n",
    "        # Create one-hot encoded tensor\n",
    "        one_hot_encoded = F.one_hot(indices, num_classes=num_classes).float()\n",
    "\n",
    "        # Apply the mask to exclude zero values\n",
    "        one_hot_encoded = one_hot_encoded * non_zero_mask.unsqueeze(-1)\n",
    "\n",
    "        # Permute dimensions to move the new dimension to the front\n",
    "        one_hot_encoded = one_hot_encoded.permute(2, 0, 1)\n",
    "        \n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return vy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "\n",
    "def expand_data_to_new_dim(input_tensor, timesteps):\n",
    "    num_classes = timesteps\n",
    "    indices = input_tensor.long()\n",
    "\n",
    "    # Create a new dimension for broadcasting\n",
    "    indices = indices.unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "    batch_size, channels, T, x, y = indices.size()  # Unpack the correct dimensions\n",
    "    one_hot_encoded = torch.zeros(batch_size, channels, num_classes, x, y)\n",
    "    one_hot_encoded.scatter_(2, indices, 1.0)\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "\n",
    "# Your 2D tensor\n",
    "input_tensor = torch.tensor([[[[1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0]]]])\n",
    "\n",
    "print(input_tensor.shape)\n",
    "timesteps = 10    # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "\n",
    "# Expand the tensor to a 3D tensor#\n",
    "#input_tensor = expand_data_to_new_dim(input_tensor, timesteps)\n",
    "#print(input_tensor.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.],\n",
      "          [0., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0., 9., 0., 0.]]]])\n",
      "tensor([[[[0., 1., 1.,  ..., 0., 1., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_tensor = torch.tensor([[[[1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0],\n",
    "                             \n",
    "                             [0.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0,1.0, 0.0, 0.0]]]])\n",
    "\n",
    "\n",
    "input_tensor = (input_tensor * timesteps) -1\n",
    "\n",
    "input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "print(input_tensor)\n",
    "# Assuming you have 'indices' and 'num_classes' defined\n",
    "num_classes = 1000\n",
    "indices = input_tensor.long()\n",
    "\n",
    "# Reshape the indices tensor for compatibility with scatter_\n",
    "reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "# Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2))\n",
    "\n",
    "# Use scatter_ to fill the one-hot encoded tensor\n",
    "one_hot_encoded.scatter_(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "# Convert the tensor to float\n",
    "one_hot_encoded = one_hot_encoded.float()\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class True3DLoss1(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        \n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class True3DLoss2(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        super().__init__()   \n",
    "        self.timesteps = timesteps\n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes = self.timesteps\n",
    "        indices = (input_tensor * self.timesteps).long().clamp(min=0, max=num_classes - 1)\n",
    "        one_hot_encoded = F.one_hot(indices, num_classes).float()\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        return true3d_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses match within the tolerance.\n",
      "Original loss: tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "Modified loss: tensor(0.0020)\n"
     ]
    }
   ],
   "source": [
    "# Create random input tensors\n",
    "torch.manual_seed(42)\n",
    "timesteps = 1000\n",
    "batch_size = 10\n",
    "height = 128\n",
    "width = 88\n",
    "channels = 1\n",
    "\n",
    "reconstructed_image = torch.rand(batch_size, channels, height, width)\n",
    "target_image = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# Calculate losses using both versions of the loss function\n",
    "original_loss_fn = True3DLoss1(timesteps)\n",
    "modified_loss_fn = True3DLoss2(timesteps)\n",
    "\n",
    "original_loss = original_loss_fn(reconstructed_image, target_image)\n",
    "modified_loss = modified_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Check if the losses are approximately equal within a tolerance\n",
    "tolerance = 1e-6\n",
    "assert torch.allclose(original_loss, modified_loss, atol=tolerance), \"Losses do not match!\"\n",
    "print(\"Losses match within the tolerance.\")\n",
    "print(\"Original loss:\", original_loss)\n",
    "print(\"Modified loss:\", modified_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Loss Function Time: 84.9020 seconds\n",
      "Modified Loss Function Time: 120.9007 seconds\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Create random input tensors\n",
    "torch.manual_seed(42)\n",
    "timesteps = 1000\n",
    "batch_size = 10\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 1\n",
    "\n",
    "reconstructed_image = torch.rand(batch_size, channels, height, width)\n",
    "target_image = torch.rand(batch_size, channels, height, width)\n",
    "\n",
    "# Create instances of the loss functions\n",
    "original_loss_fn = True3DLoss1(timesteps)\n",
    "modified_loss_fn = True3DLoss2(timesteps)\n",
    "\n",
    "# Define a function for timing the original loss function\n",
    "def time_original_loss():\n",
    "    original_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Define a function for timing the modified loss function\n",
    "def time_modified_loss():\n",
    "    modified_loss_fn(reconstructed_image, target_image)\n",
    "\n",
    "# Measure the execution time using timeit\n",
    "num_iterations = 1000\n",
    "original_time = timeit.timeit(time_original_loss, number=num_iterations)\n",
    "modified_time = timeit.timeit(time_modified_loss, number=num_iterations)\n",
    "\n",
    "print(f\"Original Loss Function Time: {original_time:.4f} seconds\")\n",
    "print(f\"Modified Loss Function Time: {modified_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:14: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_scatter\u001b[39;00m \u001b[39mimport\u001b[39;00m scatter_max\n\u001b[0;32m      4\u001b[0m src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]])\n\u001b[0;32m      5\u001b[0m index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m]])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "src = torch.tensor([[2, 0, 1, 4, 3], [0, 2, 1, 3, 4]])\n",
    "index = torch.tensor([[4, 5, 4, 2, 3], [0, 0, 2, 2, 1]])\n",
    "\n",
    "out, argmax = scatter_max(src, index, dim=-1)\n",
    "\n",
    "print(out)\n",
    "tensor([[0, 0, 4, 3, 2, 0],\n",
    "        [2, 4, 3, 0, 0, 0]])\n",
    "\n",
    "print(argmax)\n",
    "tensor([[5, 5, 3, 4, 0, 1]\n",
    "        [1, 4, 3, 5, 5, 5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class True3DLoss5(torch.nn.Module):\n",
    "    def __init__(self, timesteps=1000):\n",
    "        \"\"\"\n",
    "        # Number of dp is related to timestep size. 1,000 = 3dp, 10,000 = 4dp, 100,000 = 5dp, 1,000,000 = 6dp etc\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()   \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "\n",
    "    def expand_data_to_new_dim(self, input_tensor):\n",
    "        input_tensor = (input_tensor * self.timesteps) - 1\n",
    "        scale_time = self.start_time - time.time()\n",
    "        input_tensor = torch.where(input_tensor < 0, torch.tensor(0.0), input_tensor)   # could try input_tensor + 1.0 instead of  torch.tensor(0.0)\n",
    "        where_time = self.start_time - time.time()\n",
    "        # Assuming you have 'indices' and 'num_classes' defined\n",
    "        num_classes =  self.timesteps\n",
    "        indices = input_tensor.long()\n",
    "        long_tensor_time = self.start_time - time.time()\n",
    "        # Reshape the indices tensor for compatibility with scatter_\n",
    "        reshaped_indices = indices.view(indices.size(0), indices.size(1), -1)\n",
    "        view_time = self.start_time - time.time()\n",
    "        # Create a tensor of zeros with the same shape as 'reshaped_indices'\n",
    "        one_hot_encoded = torch.zeros(reshaped_indices.size(0), reshaped_indices.size(1), num_classes, reshaped_indices.size(2), requires_grad=True)\n",
    "        zeros_time = self.start_time - time.time()\n",
    "        # Use scatter to fill the one-hot encoded tensor (without in-place operation)\n",
    "        one_hot_encoded = one_hot_encoded.scatter(2, reshaped_indices.unsqueeze(2), 1)\n",
    "        scatter_time = self.start_time - time.time()\n",
    "        # Convert the tensor to float\n",
    "        one_hot_encoded = one_hot_encoded.float()\n",
    "        float_time = self.start_time - time.time()\n",
    "        print(f\"scale_time: {scale_time}, where_time: {where_time}, long_tensor_time: {long_tensor_time}, view_time: {view_time}, zeros_time: {zeros_time}, scatter_time: {scatter_time}, float_time: {float_time}\")\n",
    "\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward(self, reconstructed_image, target_image):\n",
    "        start_time = time.time()\n",
    "        self.start_time = start_time\n",
    "        reconstructed_3D_view = self.expand_data_to_new_dim(reconstructed_image)\n",
    "        recon3d_time = start_time - time.time()\n",
    "        target_3D_view = self.expand_data_to_new_dim(target_image)\n",
    "        target3d_time = start_time - time.time()\n",
    "        true3d_loss = self.mse_loss(reconstructed_3D_view, target_3D_view)\n",
    "        final_time = start_time - time.time()\n",
    "        print(f\"recon3d_time: {recon3d_time}, target3d_time: {target3d_time}, true3d_loss: {true3d_loss}, final_time: {final_time}\")\n",
    "\n",
    "        return true3d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = True3DLoss5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m dense_tensor[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      6\u001b[0m dense_tensor[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(dense_tensor, dense_tensor1)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[152], line 42\u001b[0m, in \u001b[0;36mTrue3DLoss5.forward\u001b[1;34m(self, reconstructed_image, target_image)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, reconstructed_image, target_image):\n\u001b[0;32m     41\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 42\u001b[0m     reconstructed_3D_view \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexpand_data_to_new_dim(reconstructed_image)\n\u001b[0;32m     43\u001b[0m     recon3d_time \u001b[39m=\u001b[39m start_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     44\u001b[0m     target_3D_view \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpand_data_to_new_dim(target_image)\n",
      "Cell \u001b[1;32mIn[152], line 17\u001b[0m, in \u001b[0;36mTrue3DLoss5.expand_data_to_new_dim\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpand_data_to_new_dim\u001b[39m(\u001b[39mself\u001b[39m, input_tensor):\n\u001b[0;32m     16\u001b[0m     input_tensor \u001b[39m=\u001b[39m (input_tensor \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimesteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 17\u001b[0m     scale_time \u001b[39m=\u001b[39m start_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     18\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(input_tensor \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m, torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m), input_tensor)   \u001b[39m# could try input_tensor + 1.0 instead of  torch.tensor(0.0)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     where_time \u001b[39m=\u001b[39m start_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "dense_tensor1 = torch.zeros(1, 1, 128, 88)\n",
    "dense_tensor = torch.zeros(1, 1, 128, 88)\n",
    "dense_tensor[0, 0, 0, 0] = 1\n",
    "dense_tensor[0, 0, 1, 1] = 1\n",
    "dense_tensor[0, 0, 2, 2] = 1\n",
    "\n",
    "loss = loss_fn(dense_tensor, dense_tensor1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
