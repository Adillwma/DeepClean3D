digraph {
	graph [size="24.15,24.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1815049400624 [label="
 (10, 1, 128, 88)" fillcolor=darkolivegreen1]
	1815049284528 [label=SigmoidBackward0]
	1815049284624 -> 1815049284528
	1815049284624 [label=ConvolutionBackward0]
	1815049283424 -> 1815049284624
	1815049283424 [label=ReluBackward0]
	1815049284768 -> 1815049283424
	1815049284768 [label=NativeBatchNormBackward0]
	1815049284864 -> 1815049284768
	1815049284864 [label=ConvolutionBackward0]
	1815049285056 -> 1815049284864
	1815049285056 [label=ReluBackward0]
	1815049285248 -> 1815049285056
	1815049285248 [label=NativeBatchNormBackward0]
	1815049285344 -> 1815049285248
	1815049285344 [label=ConvolutionBackward0]
	1815049285536 -> 1815049285344
	1815049285536 [label=ViewBackward0]
	1815049412768 -> 1815049285536
	1815049412768 [label=ReluBackward0]
	1815049412864 -> 1815049412768
	1815049412864 [label=AddmmBackward0]
	1815049412960 -> 1815049412864
	1814883528112 [label="1.decoder_lin.2.bias
 (4800)" fillcolor=lightblue]
	1814883528112 -> 1815049412960
	1815049412960 [label=AccumulateGrad]
	1815049412912 -> 1815049412864
	1815049412912 [label=ReluBackward0]
	1815049413152 -> 1815049412912
	1815049413152 [label=AddmmBackward0]
	1815049413344 -> 1815049413152
	1814883527952 [label="1.decoder_lin.0.bias
 (128)" fillcolor=lightblue]
	1814883527952 -> 1815049413344
	1815049413344 [label=AccumulateGrad]
	1815049413296 -> 1815049413152
	1815049413296 [label=AddmmBackward0]
	1815049413488 -> 1815049413296
	1814883527792 [label="0.encoder_lin.2.bias
 (10)" fillcolor=lightblue]
	1814883527792 -> 1815049413488
	1815049413488 [label=AccumulateGrad]
	1815049413536 -> 1815049413296
	1815049413536 [label=ReluBackward0]
	1815049413680 -> 1815049413536
	1815049413680 [label=AddmmBackward0]
	1815049413872 -> 1815049413680
	1814883527632 [label="0.encoder_lin.0.bias
 (128)" fillcolor=lightblue]
	1814883527632 -> 1815049413872
	1815049413872 [label=AccumulateGrad]
	1815049413824 -> 1815049413680
	1815049413824 [label=ReshapeAliasBackward0]
	1815049414016 -> 1815049413824
	1815049414016 [label=ReluBackward0]
	1815049414208 -> 1815049414016
	1815049414208 [label=ConvolutionBackward0]
	1815049414304 -> 1815049414208
	1815049414304 [label=ReluBackward0]
	1815049414496 -> 1815049414304
	1815049414496 [label=NativeBatchNormBackward0]
	1815049414592 -> 1815049414496
	1815049414592 [label=ConvolutionBackward0]
	1815049414784 -> 1815049414592
	1815049414784 [label=ReluBackward0]
	1815049414976 -> 1815049414784
	1815049414976 [label=ConvolutionBackward0]
	1815049415024 -> 1815049414976
	1814883526512 [label="0.encoder_cnn.0.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	1814883526512 -> 1815049415024
	1815049415024 [label=AccumulateGrad]
	1815049414880 -> 1815049414976
	1814883526592 [label="0.encoder_cnn.0.bias
 (8)" fillcolor=lightblue]
	1814883526592 -> 1815049414880
	1815049414880 [label=AccumulateGrad]
	1815049414736 -> 1815049414592
	1814883526752 [label="0.encoder_cnn.2.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1814883526752 -> 1815049414736
	1815049414736 [label=AccumulateGrad]
	1815049414688 -> 1815049414592
	1814883526832 [label="0.encoder_cnn.2.bias
 (16)" fillcolor=lightblue]
	1814883526832 -> 1815049414688
	1815049414688 [label=AccumulateGrad]
	1815049414544 -> 1815049414496
	1814883526912 [label="0.encoder_cnn.3.weight
 (16)" fillcolor=lightblue]
	1814883526912 -> 1815049414544
	1815049414544 [label=AccumulateGrad]
	1815049414400 -> 1815049414496
	1814883526992 [label="0.encoder_cnn.3.bias
 (16)" fillcolor=lightblue]
	1814883526992 -> 1815049414400
	1815049414400 [label=AccumulateGrad]
	1815049414256 -> 1815049414208
	1814883527392 [label="0.encoder_cnn.5.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1814883527392 -> 1815049414256
	1815049414256 [label=AccumulateGrad]
	1815049414112 -> 1815049414208
	1814883527472 [label="0.encoder_cnn.5.bias
 (32)" fillcolor=lightblue]
	1814883527472 -> 1815049414112
	1815049414112 [label=AccumulateGrad]
	1815049413776 -> 1815049413680
	1815049413776 [label=TBackward0]
	1815049414352 -> 1815049413776
	1814883527552 [label="0.encoder_lin.0.weight
 (128, 4800)" fillcolor=lightblue]
	1814883527552 -> 1815049414352
	1815049414352 [label=AccumulateGrad]
	1815049413584 -> 1815049413296
	1815049413584 [label=TBackward0]
	1815049414160 -> 1815049413584
	1814883527712 [label="0.encoder_lin.2.weight
 (10, 128)" fillcolor=lightblue]
	1814883527712 -> 1815049414160
	1815049414160 [label=AccumulateGrad]
	1815049413248 -> 1815049413152
	1815049413248 [label=TBackward0]
	1815049414448 -> 1815049413248
	1814883527872 [label="1.decoder_lin.0.weight
 (128, 10)" fillcolor=lightblue]
	1814883527872 -> 1815049414448
	1815049414448 [label=AccumulateGrad]
	1815049412672 -> 1815049412864
	1815049412672 [label=TBackward0]
	1815049413920 -> 1815049412672
	1814883528032 [label="1.decoder_lin.2.weight
 (4800, 128)" fillcolor=lightblue]
	1814883528032 -> 1815049413920
	1815049413920 [label=AccumulateGrad]
	1815049285488 -> 1815049285344
	1814883528352 [label="1.decoder_conv.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	1814883528352 -> 1815049285488
	1815049285488 [label=AccumulateGrad]
	1815049285440 -> 1815049285344
	1814883528432 [label="1.decoder_conv.0.bias
 (16)" fillcolor=lightblue]
	1814883528432 -> 1815049285440
	1815049285440 [label=AccumulateGrad]
	1815049285296 -> 1815049285248
	1814883528512 [label="1.decoder_conv.1.weight
 (16)" fillcolor=lightblue]
	1814883528512 -> 1815049285296
	1815049285296 [label=AccumulateGrad]
	1815049285152 -> 1815049285248
	1814883528592 [label="1.decoder_conv.1.bias
 (16)" fillcolor=lightblue]
	1814883528592 -> 1815049285152
	1815049285152 [label=AccumulateGrad]
	1815049285008 -> 1815049284864
	1814883611008 [label="1.decoder_conv.3.weight
 (16, 8, 3, 3)" fillcolor=lightblue]
	1814883611008 -> 1815049285008
	1815049285008 [label=AccumulateGrad]
	1815049284960 -> 1815049284864
	1814883611088 [label="1.decoder_conv.3.bias
 (8)" fillcolor=lightblue]
	1814883611088 -> 1815049284960
	1815049284960 [label=AccumulateGrad]
	1815049284816 -> 1815049284768
	1814883611168 [label="1.decoder_conv.4.weight
 (8)" fillcolor=lightblue]
	1814883611168 -> 1815049284816
	1815049284816 [label=AccumulateGrad]
	1815049284672 -> 1815049284768
	1814883611248 [label="1.decoder_conv.4.bias
 (8)" fillcolor=lightblue]
	1814883611248 -> 1815049284672
	1815049284672 [label=AccumulateGrad]
	1815049282560 -> 1815049284624
	1814883611648 [label="1.decoder_conv.6.weight
 (8, 1, 3, 3)" fillcolor=lightblue]
	1814883611648 -> 1815049282560
	1815049282560 [label=AccumulateGrad]
	1815049282656 -> 1815049284624
	1814883611728 [label="1.decoder_conv.6.bias
 (1)" fillcolor=lightblue]
	1814883611728 -> 1815049282656
	1815049282656 [label=AccumulateGrad]
	1815049284528 -> 1815049400624
}
