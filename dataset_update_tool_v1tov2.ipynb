{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 Dataset Format\n",
    "\n",
    "# This document describes the V2 dataset format. This format is used by the updated Dataloader for increased speed especially on large datasets.\n",
    "\n",
    "# TL:DR\n",
    "files on disk now contain 1000 samples not 1\n",
    "\n",
    "Files on disk are now tensors not numpy arrays\n",
    "\n",
    "Files on disk now have shape [1000, 1, 128, 88] not [128, 88]\n",
    "\n",
    "Datsets now have minimum size of 1000 samples. And total samples must be a multiple of 1000?\n",
    "\n",
    "# CHANGES FROM V1\n",
    "- Instead of each file being saved on disk as a single numpy array, files are now saved in 'bundles' of 1000 samples each. This allows for faster loading of the dataset by reducing load calls from disk by 1000x which was a previous bottleneck.\n",
    "\n",
    "- data is now saved to disk as tensors instead of numpy arrays. removing the need for conversion to tensors in the dataloader.???\n",
    "\n",
    "- Channel dim is already included in the data. This means that the data is now of shape (X, C, Y)????(C, X, Y) instead of (X, Y). Which removes this step from the dataloader.\n",
    "\n",
    "- Sparse??\n",
    "\n",
    "- NPZ support??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d9dfb94b1b4d4b8ff386694d32dc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing bundles:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All bundles processed and saved successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# User Inputs\n",
    "bundle_size = 10000    # 1K for V2 Spec, 10K for V3\n",
    "dataset_full_filepath = r\"N:\\Yr 3 Project Datasets\\PDT 10K\\\\Data\\\\\"\n",
    "dataset_output_filepath = r\"N:\\Yr 3 Project Datasets\\\\V3_PDT_10K\\\\Data\\\\\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(dataset_output_filepath):\n",
    "    os.makedirs(dataset_output_filepath)\n",
    "else:\n",
    "    raise Exception(\"Output directory already exists, please delete it before running this script\")\n",
    "\n",
    "# Get a list of all the files in the dataset\n",
    "dataset_files = os.listdir(dataset_full_filepath)\n",
    "\n",
    "# Create batches of 'bundle_size' file paths and add each batch to a list\n",
    "file_paths_on_disk = [dataset_files[i:i+bundle_size] for i in range(0, len(dataset_files), bundle_size)]\n",
    "\n",
    "def process_bundle(bundle_idx, batch_of_file_paths_on_disk):\n",
    "    # Stack 'bundle_size' numpy arrays into one array\n",
    "    data_bundle = np.stack([np.load(os.path.join(dataset_full_filepath, file_path)) for file_path in batch_of_file_paths_on_disk], axis=0)\n",
    "\n",
    "    # Convert numpy array to tensor\n",
    "    data_bundle = torch.tensor(data_bundle, dtype=torch.float64)\n",
    "\n",
    "    # Add channel dim\n",
    "    data_bundle = data_bundle.unsqueeze(1)\n",
    "\n",
    "    # Name file\n",
    "    file_name = f'test_{bundle_idx}.pt'\n",
    "\n",
    "    # Save bundle to disk\n",
    "    torch.save(data_bundle, os.path.join(dataset_output_filepath, file_name))\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    list(tqdm(executor.map(lambda args: process_bundle(*args), enumerate(file_paths_on_disk)), total=len(file_paths_on_disk), desc=\"Processing bundles\", colour=\"yellow\"))\n",
    "\n",
    "\n",
    "# make sure all threads are done\n",
    "executor.shutdown(wait=True)\n",
    "\n",
    "print(\"All bundles processed and saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'N:\\\\Yr 3 Project Datasets\\\\\\\\SPEEDTESTDEL1_[V3]_RDT_50K\\\\Data\\\\\\\\test_50.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# load and compare the first 10 files\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     file1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_output_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     file2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_output_filepath2, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(file1, file2), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Ada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'N:\\\\Yr 3 Project Datasets\\\\\\\\SPEEDTESTDEL1_[V3]_RDT_50K\\\\Data\\\\\\\\test_50.pt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
